{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a738ec7b-2279-40ed-9b41-74862022f8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我的创造者是摩尔线程的AI科学家。他们基于摩尔GPU的AI计算和渲染功能，赋予了我丰富的能力。我会智能内容创作，比如对对联，吟诗作画。身后的画就是我创作的，对AI艺术感兴趣的朋友们，一会儿可以去画廊参观。', '你的父母是谁？', 0.872, '你的爸爸是谁？')\n"
     ]
    }
   ],
   "source": [
    "#FAQ\n",
    "import grpc\n",
    "from protos.nlp.faq import faq_pb2 as pb2\n",
    "from protos.nlp.faq import faq_pb2_grpc as pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "class Faq_inference():\n",
    "    def __init__(self):\n",
    "        super(Faq_inference, self).__init__()\n",
    "\n",
    "    def faq_answer(self, query):\n",
    "        # conn = grpc.insecure_channel('192.168.4.29:60009')\n",
    "        conn = grpc.insecure_channel('172.31.208.10:58999')\n",
    "        # conn = grpc.insecure_channel('172.31.208.10:58999')\n",
    "        client = pb2_grpc.FaqStub(conn)\n",
    "        res = client.GetFaq(pb2.FaqRequest(query=query, robot_id=1), timeout=1)\n",
    "        answer = res.answer #faq对应的答案\n",
    "        match = res.match   #匹配到的相似问句\n",
    "        faq = res.faq       #对应的标准问句\n",
    "        score = res.score   #匹配模型匹配的分数，可以视为置信度\n",
    "        return answer, faq, score, match\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"我是你爸爸\"\n",
    "    tt = Faq_inference()\n",
    "    print(tt.faq_answer(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb5bdc-a5fe-44a0-8811-11a425e7cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 意图识别\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "# 读取csv文件(前n行)\n",
    "def read(train_set_path, test_set_path):\n",
    "    data_1 = pd.read_csv('./faq.txt', sep='\\t', header=None, names=['query', 'faq', 'answer','tts'], usecols = ['query'])\n",
    "    data_1['class'] = 0\n",
    "    data_temp = pd.read_csv('qingyun_chat.txt', header=None, names=['query'])\n",
    "    data_temp['class'] = 1\n",
    "    data_2 = pd.read_csv('./chat.txt', sep='\\t', header=None, names=['match', 'query', 'answer','tts'], usecols = ['query'])\n",
    "    data_2['class'] = 1\n",
    "    data_3 = pd.read_csv('docqa.csv', header=None, names = ['query'])\n",
    "    data_3['class'] = 2\n",
    "    data_1 = data_1.drop_duplicates()\n",
    "    data_2 = data_2.drop_duplicates()\n",
    "    data = pd.concat((data_1, data_2, data_3, data_temp))\n",
    "    \n",
    "    data['query'] = data['query'].apply(lambda x: ' '.join(str(x)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['query'], data['class'], test_size=0.15)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 训练拟合模型\n",
    "def train(X_train, y_train):\n",
    "    vectorizer = TfidfVectorizer(max_features=800000, min_df=2, analyzer='word', token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", ngram_range=(1, 5)) \n",
    "    x_train = vectorizer.fit_transform(X_train)    \n",
    "    # 构建逻辑回归模型\n",
    "    logistic_model = LogisticRegression(C=3, dual=False) \n",
    "    # 拟合,训练\n",
    "    logistic_model.fit(x_train, y_train) \n",
    "    \n",
    "    tfidftransformer_path = 'models/tfidftransformer.pkl'\n",
    "    with open(tfidftransformer_path, 'wb') as fw:\n",
    "         pickle.dump(vectorizer, fw)        \n",
    "    joblib.dump(logistic_model, \"models/logisitic_model.pkl\") \n",
    "    return vectorizer, logistic_model\n",
    "\n",
    "\n",
    "# 保存预测结果\n",
    "def save(test_data, y_test, save_path):\n",
    "    # 保存结果\n",
    "    y_test = y_test.tolist()  # 把(n,)的一维矩阵y_test转化为矩阵列表形式存储\n",
    "    test_data['class'] = y_test + 1  # 新建一个class列,存入类别值(类别还原+1)\n",
    "    data_result = test_data.loc[:, ['id', 'class']]  # 根据index索引所有行和id,class列\n",
    "    data_result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    chars = '*' * 20\n",
    "    '''1.读取数据'''\n",
    "    # old_time = time.time() \n",
    "    X_train, X_test, y_train, y_test = read('./faq.txt', './chat.txt')\n",
    "    # current_time = time.time()\n",
    "    # print(chars + '读取完成!用时{}s'.format(round(current_time-old_time,2)) + chars + '\\n')\n",
    "\n",
    "    '''2.训练'''\n",
    "    vectorizer, logistic_model = train(X_train, y_train)\n",
    "    \n",
    "    vectorizer = pickle.load(open('models/tfidftransformer.pkl', \"rb\"))\n",
    "    logistic_model = joblib.load('models/logisitic_model.pkl')\n",
    "    \n",
    "    '''3.预测'''\n",
    "    x_test = vectorizer.transform(X_test)\n",
    "    y_pred = logistic_model.predict(x_test) # (n,)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(acc_score)\n",
    "    F1_score = f1_score(y_test, y_pred, average='macro')\n",
    "    print(F1_score)\n",
    "    \n",
    "    \n",
    "    #0:闲聊 ， 1:客服， 2: 文档问答（知识问答）\n",
    "    C2 = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "    # 计算百分比\n",
    "    C2_normal = np.round(C2/np.sum(C2, axis=1).reshape(-1, 1), 2)\n",
    "\n",
    "    sns.set()\n",
    "    f, ax = plt.subplots()\n",
    "    sns.heatmap(C2_normal, cmap=\"YlGnBu\", annot=True, ax=ax) # heatmap\n",
    "\n",
    "    #ax.set_title('Normalized confusion maxtrix') # title\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.xaxis.set_ticklabels([0, 1, 2])\n",
    "    ax.yaxis.set_ticklabels([0, 1, 2])\n",
    "    \n",
    "    print('Over!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e295b-1bb5-42cb-9a92-0a6d20d04015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping = {-1:'Unknown', 0:'Faq', 1:'Chat', 2:'DocQA', 3:'Mutil-Turn'}\n",
    "import pickle\n",
    "import joblib\n",
    "query = '她的妻子叫什么'\n",
    "vectorizer = pickle.load(open('models/tfidftransformer.pkl', \"rb\"))\n",
    "model = joblib.load('models/logisitic_model.pkl')\n",
    "pred_class = model.predict(vectorizer.transform([' '.join(query)]))\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc05e1d4-fa53-4eae-a2d9-820e132fd871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************回复完成!用时0.67s********************\n",
      "results is: 你好,请问你是? 0.5666664776421106 你好 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from protos.nlp.chat import chat_pb2\n",
    "from protos.nlp.chat import chat_pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "\n",
    "\n",
    "class Chat_inference(object):\n",
    "    def __init__(self):\n",
    "        super(Chat_inference, self).__init__()\n",
    "\n",
    "    def response(self, history_utts, query):\n",
    "        conn = implementations.insecure_channel(\"172.31.208.9\", 60005)\n",
    "        client = chat_pb2_grpc.ChatStub(channel=conn._channel)\n",
    "        res = client.GetChat(chat_pb2.ChatRequest(history=history_utts, query=query, session_id='1', robot_id=1), timeout=60)\n",
    "        answer = res.answer\n",
    "        score = res.score\n",
    "        query = res.query\n",
    "        sessionid = res.session_id\n",
    "        return answer, score, query, sessionid\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_infer = Chat_inference()\n",
    "    \n",
    "    history_utts = [{\"conversation\": [],'intent':[],'slots':[],'entities':[],'task_id':[]}]\n",
    "    query = \"你好\"\n",
    "    start_time = time.time()\n",
    "    results = chat_infer.response(history_utts, query)\n",
    "    end_time = time.time()\n",
    "    print('*' * 20 + '回复完成!用时{:.2f}s'.format(end_time - start_time) + '*' * 20)\n",
    "    print('results is:', *results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24aeb4-4d3d-4015-888e-71247deecfca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DocQa\n",
    "from protos.nlp.docqa import docqa_pb2\n",
    "from protos.nlp.docqa import docqa_pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "\n",
    "class DocQAInference(object):\n",
    "    def __init__(self, config):\n",
    "        self.host = config.get('host', \"172.31.208.4\")\n",
    "        self.port = config.get('port', 60005)\n",
    "        # self.port = config.get('port', 6082)\n",
    "        self.timeout = config.get('timeout', 60)\n",
    "\n",
    "    def docqa(self, index_id, query):\n",
    "        conn = implementations.insecure_channel(self.host, self.port)\n",
    "        client = docqa_pb2_grpc.DocqaStub(channel=conn._channel)\n",
    "        res = client.GetDocqa(docqa_pb2.DocqaRequest(index_id=index_id, query=query, session_id='1', robot_id=1), timeout=self.timeout)\n",
    "        return res\n",
    "\n",
    "#‘诗句：‘夜来风雨声’的下一句是什么’\n",
    "if __name__ == \"__main__\":\n",
    "    query_index_list = [\n",
    "        (\"中国有多少个省\", \"wiki\"),\n",
    "    ]\n",
    "    query_list = [item[0] for item in query_index_list]\n",
    "    index_list = [item[1] for item in query_index_list]\n",
    "    # index_list = index_list*25\n",
    "    # query_list = query_list*25\n",
    "    \n",
    "    inference = DocQAInference({})\n",
    "    inference.docqa(index_list[0], query_list[0])\n",
    "\n",
    "    import numpy as np\n",
    "    import time\n",
    "    query_lens = []\n",
    "    latency_list = []\n",
    "    for index, query in zip(index_list, query_list):\n",
    "        print(f\"index={index}\")\n",
    "        print(f\"query={query}\")\n",
    "        start_time = time.time()    \n",
    "        docqa_rst = inference.docqa(index, query)\n",
    "        time_cost = time.time() - start_time\n",
    "        print(f\"result={MessageToJson(docqa_rst, ensure_ascii=False, indent=2)}\")\n",
    "        latency_list.append(time_cost*1000)\n",
    "        query_len = len(query)\n",
    "        query_lens.append(query_len)\n",
    "        \n",
    "                \n",
    "    print(f\"mean of query len is {np.mean(query_lens)}\")\n",
    "    print(f\"mean of latency is {np.mean(latency_list)}ms\")\n",
    "    print(f\"0.95 of latency is {np.quantile(latency_list, 0.95)}ms\")\n",
    "    print(f\"0.99 of latency is {np.quantile(latency_list, 0.99)}ms\")\n",
    "    \n",
    "#特朗普的弟弟叫什么\n",
    "#碧昂斯的出生地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe830df7-fd27-4e2e-88db-da678ef37c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"squad_data.json\", encoding='utf-8', errors='ignore') as json_data:\n",
    "     data = json.load(json_data, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5717-5efc-48d4-a0f5-47a858540adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a46c2-f5c7-41d7-968c-ce1bdec2f164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = []\n",
    "for i in range(len(data['data'])):\n",
    "    for j in range(len(data['data'][i]['paragraphs'])):\n",
    "        for z in data['data'][i]['paragraphs'][j]['qas']:\n",
    "            query.append(z['question'])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1bc91-f5b5-4a39-ba57-c45f1947fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca732f5-a0cb-42bc-ad5a-dc6564b7ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('docqa_1.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4034ee3e-7b79-42fa-92d9-3877a0006279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天的的呢 0.815974235534668\n",
      "明天的天气呢 0.802768886089325\n",
      "明天的天气呢 0.8084323406219482\n",
      "明天的的呢\n"
     ]
    }
   ],
   "source": [
    "#改写模块\n",
    "import grpc\n",
    "import sys\n",
    "from protos.nlp.rewrite import rewrite_pb2\n",
    "from protos.nlp.rewrite import rewrite_pb2_grpc\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "from grpc.beta import implementations\n",
    "\n",
    "class IUR_inference(object):\n",
    "    def __init__(self, config):\n",
    "        self.host = config.get('host', \"192.168.68.26\")\n",
    "        self.port = config.get('port', 60005)\n",
    "        self.timeout = config.get('timeout', 60)\n",
    "\n",
    "    def iur(self, history_utts, query):\n",
    "        conn = implementations.insecure_channel(self.host, self.port)\n",
    "        client = rewrite_pb2_grpc.ReWriteStub(channel=conn._channel)\n",
    "        res = client.GetReWrite(rewrite_pb2.ReWriteRequest(history=history_utts, query=query, session_id='1', robot_id=1), timeout=self.timeout)\n",
    "        new_query = res.new_query\n",
    "        score = res.score\n",
    "        return new_query, score\n",
    "\n",
    "    def query_write_strategy(self, query, conversation):\n",
    "        new_query_1, qr_score_1 = self.iur(conversation[:2], query)\n",
    "        new_query_2, qr_score_2 = self.iur(conversation[2:], query)\n",
    "        new_query_3, qr_score_3 = self.iur(conversation, query)\n",
    "        new_query, qr_score = new_query_1, qr_score_1\n",
    "        if qr_score_1 <qr_score_2:\n",
    "            new_query = new_query_2\n",
    "            qr_score = qr_score_2\n",
    "        print(new_query_1, qr_score_1)\n",
    "        print(new_query_2, qr_score_2)\n",
    "        print(new_query_3, qr_score_3)\n",
    "        if qr_score>0.8 and new_query!=query:\n",
    "            return new_query\n",
    "        return query\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # history_utts = [\"\", \"\", \"北京可以办理工作居住证么？\", \"可以，根据政府政策规定，入职满一年后满足条件的可办理。\"]\n",
    "    # query = \"多久能下来？\"\n",
    "    # history_utts = [\"\", \"\", \"有松下的吹风机吗\", \"没有哦亲\"]\n",
    "    # query = \"那有戴森的吗\"\n",
    "    # history_utts = [\"\", \"\", \"帮我放周杰伦的歌\", \"好的，正在播放周杰伦的稻香\"]\n",
    "    # query = \"算了，不要放他的，放张学友的\"\n",
    "    # history_utts = ['','']\n",
    "    # history_utts = ['你知道碧昂斯吗', '肯定知道啊','那你知道她的第一张专辑吗','危险爱情']\n",
    "    # history_utts = ['你知道碧昂斯吗','我的朋友有很多啊，你也是我朋友啊你忘啦？', '她的第一张专辑是什么', '危险爱情']\n",
    "    history_utts = ['','','今天的天气怎么样', '阴天,有点闷热']\n",
    "    # history_utts = ['怎么申请设备呢', 'IT标准库存资产在金蝶系统-人人资产填写申请；IT标准配件在飞书-工作台-审批填写耗材领用流程；非标准库存配件在金蝶系统填写采购申请。', '那北京可以办理居住证吗', '可以，根据政府政策规定，入职满一年后满足条件的可办理。']\n",
    "    # history_utts = ['怎么申请设备', 'IT标准库存资产在金蝶系统-人人资产填写申请；IT标准配件在飞书-工作台-审批填写耗材领用流程；非标准库存配件在金蝶系统填写采购申请。', '北京可以办理居住证吗', '可以，需上家公司减员后绑定公司关联码后系统提交材料即可。']\n",
    "    # history_utts = ['你好', '你好呀，我可以帮你做点啥嘛', '你认识碧昂斯吗', '那就多了去了，亲人朋友名人数不胜数呐！']\n",
    "    # query=\"她的第一张专辑是什么\"\n",
    "    query = '明天的呢'\n",
    "    tt = IUR_inference({})\n",
    "    new_query = tt.query_write_strategy(query, history_utts)\n",
    "    # new_query, score = tt.iur(history_utts, query)\n",
    "    print(new_query)\n",
    "#0.9014959335327148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a32171-dca4-4f2a-b351-8fcf9082a506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/0p/cjvwlkfs59d5t070gww_cp300000gn/T/jieba.cache\n",
      "Loading model cost 0.284 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import jieba.posseg as psg\n",
    "import jieba\n",
    "jieba.add_word('今天', 100000)\n",
    "jieba.add_word('明天', 100000)\n",
    "jieba.add_word('后天', 100000)\n",
    "jieba.add_word('上午',100000)\n",
    "jieba.add_word('下午',100000)\n",
    "\n",
    "def get_time_word(text):\n",
    "    time_res = []\n",
    "    word = ''\n",
    "    keyDate = {'今天': 0, '明天':1, '后天': 2}\n",
    "    for k, v in psg.cut(text):\n",
    "        if k in keyDate:\n",
    "            if word != '':\n",
    "                time_res.append(word)\n",
    "            time_res.append(k)\n",
    "        elif word != '':\n",
    "            if v in ['m', 't']:\n",
    "                word = word + k\n",
    "            else:\n",
    "                time_res.append(word)\n",
    "                word = ''\n",
    "        elif v in ['m', 't'] and '张' not in k:\n",
    "            word = k\n",
    "    if word != '':\n",
    "        time_res.append(word)\n",
    "    return time_res\n",
    "\n",
    "def time_extract(text):\n",
    "    # text = text\n",
    "    time_res = []\n",
    "    word = ''\n",
    "    keyDate = {'今天': 0, '明天':1, '后天': 2}\n",
    "    for k, v in psg.cut(text):\n",
    "        if k in keyDate:\n",
    "            if word != '':\n",
    "                time_res.append(word)\n",
    "            word = (datetime.today() + timedelta(days=keyDate.get(k, 0))).strftime('%Y年%m月%d日')\n",
    "        elif word != '':\n",
    "            if v in ['m', 't']:\n",
    "                word = word + k\n",
    "            else:\n",
    "                time_res.append(word)\n",
    "                word = ''\n",
    "        elif v in ['m', 't'] and '张' not in k:\n",
    "            word = k\n",
    "    if word != '':\n",
    "        time_res.append(word)\n",
    "    result = list(filter(lambda x: x is not None, [check_time_valid(w) for w in time_res]))\n",
    "    final_res = [parse_datetime(w) for w in result]\n",
    "    return [x for x in final_res if x is not None]\n",
    "\n",
    "def check_time_valid(word):\n",
    "    m = re.match(\"\\d+$\", word)\n",
    "    if m:\n",
    "        if len(word) <= 6:\n",
    "            return None\n",
    "    word1 = re.sub('[号|日]\\d+$', '日', word)\n",
    "    if word1 != word:\n",
    "        return check_time_valid(word1)\n",
    "    else:\n",
    "        return word1\n",
    "    \n",
    "def parse_datetime(msg):\n",
    "    if msg is None or len(msg) == 0:\n",
    "        return None\n",
    "\n",
    "    if '午' not in msg and '点' not in msg:\n",
    "        dt = parse(msg, fuzzy=True)\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        m = re.match(\n",
    "            r\"([0-9零一二两三四五六七八九十]+年)?([0-9一二两三四五六七八九十]+月)?([0-9一二两三四五六七八九十]+[号日])?([上中下午晚早]+)?([0-9零一二两三四五六七八九十百]+[点:\\.时])?([0-9零一二三四五六七八九十百]+分?)?([0-9零一二三四五六七八九十百]+秒)?\",\n",
    "            msg)\n",
    "        if m.group(0) is not None:\n",
    "            res = {\n",
    "                \"year\": m.group(1),\n",
    "                \"month\": m.group(2),\n",
    "                \"day\": m.group(3),\n",
    "                \"hour\": m.group(5) if m.group(5) is not None else '00',\n",
    "                \"minute\": m.group(6) if m.group(6) is not None else '00',\n",
    "                \"second\": m.group(7) if m.group(7) is not None else '00',\n",
    "            }\n",
    "            params = {}\n",
    "\n",
    "            for name in res:\n",
    "                if res[name] is not None and len(res[name]) != 0:\n",
    "                    tmp = None\n",
    "                    if name == 'year':\n",
    "                        tmp = year2dig(res[name][:-1])\n",
    "                    else:\n",
    "                        tmp = cn2dig(res[name][:-1])\n",
    "                    if tmp is not None:\n",
    "                        params[name] = int(tmp)\n",
    "            target_date = datetime.today().replace(**params)\n",
    "            is_pm = m.group(4)\n",
    "            if is_pm is not None:\n",
    "                if is_pm == u'下午' or is_pm == u'晚上' or is_pm =='中午':\n",
    "                    hour = target_date.time().hour\n",
    "                    if hour < 12:\n",
    "                        target_date = target_date.replace(hour=hour + 12)\n",
    "            return target_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "UTIL_CN_NUM = {\n",
    "    '零': 0, '一': 1, '二': 2, '两': 2, '三': 3, '四': 4,\n",
    "    '五': 5, '六': 6, '七': 7, '八': 8, '九': 9,\n",
    "    '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n",
    "    '5': 5, '6': 6, '7': 7, '8': 8, '9': 9\n",
    "}\n",
    "UTIL_CN_UNIT = {'十': 10, '百': 100, '千': 1000, '万': 10000}\n",
    "def cn2dig(src):\n",
    "    if src == \"\":\n",
    "        return None\n",
    "    m = re.match(\"\\d+\", src)\n",
    "    if m:\n",
    "        return int(m.group(0))\n",
    "    rsl = 0\n",
    "    unit = 1\n",
    "    for item in src[::-1]:\n",
    "        if item in UTIL_CN_UNIT.keys():\n",
    "            unit = UTIL_CN_UNIT[item]\n",
    "        elif item in UTIL_CN_NUM.keys():\n",
    "            num = UTIL_CN_NUM[item]\n",
    "            rsl += num * unit\n",
    "        else:\n",
    "            return None\n",
    "    if rsl < unit:\n",
    "        rsl += unit\n",
    "    return rsl\n",
    "def year2dig(year):\n",
    "    res = ''\n",
    "    for item in year:\n",
    "        if item in UTIL_CN_NUM.keys():\n",
    "            res = res + str(UTIL_CN_NUM[item])\n",
    "        else:\n",
    "            res = res + item\n",
    "    m = re.match(\"\\d+\", res)\n",
    "    if m:\n",
    "        if len(m.group(0)) == 2:\n",
    "            return int(datetime.datetime.today().year/100)*100 + int(m.group(0))\n",
    "        else:\n",
    "            return int(m.group(0))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a037a0-6596-44da-8bc1-5b79f694c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['time'], ['2023-03-11 20:00:00'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def ner(query):\n",
    "    entities = []\n",
    "    slots = []\n",
    "    query = query.replace('今晚','今天晚上').replace('明早','明天早上').replace('明晚','明天晚上')\n",
    "\n",
    "    time = time_extract(query)\n",
    "    if time:\n",
    "        slots.append('time')\n",
    "        entities.append(time[0])\n",
    "    time_word = get_time_word(query)\n",
    "    for word in time_word:\n",
    "        query = query.replace(word, '')\n",
    "    print(query)\n",
    "\n",
    "    m = re.findall(r\"订.*?[0-9一二两三四五六七八九十]?张?从?([\\u4e00-\\u9fa5]{2,5}?)?(出发)?[到｜飞|去｜]([\\u4e00-\\u9fa5]{2,5}?)的\",query)\n",
    "    if m:\n",
    "        start_city = m[0][0]\n",
    "        end_city = m[0][2]\n",
    "        if start_city:\n",
    "            slots.append('start_city')\n",
    "            entities.append(start_city)\n",
    "        if end_city:\n",
    "            slots.append('end_city')\n",
    "            entities.append(end_city)\n",
    "\n",
    "        print(start_city)\n",
    "        print(end_city)\n",
    "    else:\n",
    "        pass\n",
    "    return slots, entities\n",
    "    \n",
    "ner('明晚八点')\n",
    "# ner('订一张明晚八点的机票')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27090e6-c728-4693-b418-cce61c6c14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '帮我订一张明晚八点上海到深圳的机票'\n",
    "print(text1, time_extract(text1), sep=':')\n",
    "\n",
    "# text2 = '预定28号的房间'\n",
    "# print(text2, time_extract(text2), sep=':')\n",
    "\n",
    "# text3 = '我要从26号下午4点住到11月2号'\n",
    "# print(text3, time_extract(text3), sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc8c6f-2914-432d-912c-d16499502b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#情绪识别\n",
    "\n",
    "import grpc\n",
    "from protos.nlp.faq import faq_pb2, faq_pb2_grpc\n",
    "from protos.nlp.emotion import emotion_pb2, emotion_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('172.31.208.10:58999')\n",
    "texts = ['你真棒']\n",
    "\n",
    "# emotion\n",
    "emotion_client = emotion_pb2_grpc.EmotionStub(channel)\n",
    "for query in texts:\n",
    "    resp = emotion_client.GetEmotion(emotion_pb2.EmotionRequest(query=query, session_id='1', robot_id=2))\n",
    "    # print(resp.answer.happy, resp.answer.sad, resp.answer.angry, resp.answer.disgust,\n",
    "    #       resp.answer.fear, resp.answer.surprise, resp.answer.none)\n",
    "    print(resp)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01108689-0d3d-4c6e-b3b2-9e0182e6c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  57510\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2868579 lr:  0.000000 avg.loss:  0.072843 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17533, 0.9755888895226145, 0.9755888895226145)\n",
      "周杰伦 爸爸 叫 什么\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#领域分类模型（Fasttext）\n",
    "import fasttext\n",
    "import jieba\n",
    "# from gensim.models import FastText\n",
    "jieba.add_word('你知道',100000)\n",
    "\n",
    "model = fasttext.train_supervised(input = r\"./train.txt\", label_prefix=\"__label__\", epoch=10, verbose=2, lr=0.04, loss=\"softmax\")\n",
    "\n",
    "model.save_model(r\"models/model.bin\")\n",
    "\n",
    "stop_words = ['的','了','吗','啊','拉','呢','你知道','告诉', '请问']\n",
    "\n",
    "def word_segment(text):\n",
    "    seg_list = jieba.cut(text)\n",
    "    word_list = []\n",
    "    for word in seg_list:\n",
    "        if word not in stop_words:\n",
    "            word_list.append(word)\n",
    "    line = \" \".join(word_list)\n",
    "    print(line)\n",
    "    return line\n",
    "\n",
    "# 预测\n",
    "model = fasttext.load_model(r\"models/model.bin\")\n",
    "print(model.test(r\"./test.txt\")  )\n",
    "\n",
    "# text = 'hi穆莎'\n",
    "text = '你知道周杰伦的爸爸叫什么吗'\n",
    "print(model.predict(word_segment(text))[0][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943ca756-1fa3-48f1-b2b5-ac605ecc9960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 1256, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Faq\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./faq.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m data_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#闲聊\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 1256, saw 4\n"
     ]
    }
   ],
   "source": [
    "#文件预处理\n",
    "import jieba\n",
    "jieba.add_word('你知道',100000)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Faq\n",
    "data_1 = pd.read_csv('./faq.txt', sep='\\t', header=None, names=['query'])\n",
    "data_1['class'] = 0\n",
    "\n",
    "#闲聊\n",
    "data_temp = pd.read_csv('qingyun_chat.txt', header=None, names=['query'])[:50000]\n",
    "data_temp['class'] = 1\n",
    "data_2 = pd.read_csv('./chat.txt', sep='\\t', header=None, names=['query'])\n",
    "data_2['class'] = 1\n",
    "\n",
    "#文档问答\n",
    "data_3 = pd.read_csv('docqa_2.csv', header=None, names = ['query'])\n",
    "data_3['class'] = 2\n",
    "\n",
    "data_2 = data_2.drop_duplicates()\n",
    "data_temp = data_temp.drop_duplicates()\n",
    "data = pd.concat((data_1, data_2, data_3, data_temp))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['query'], data['class'], test_size=0.15)\n",
    "\n",
    "stop_words = ['你知道','的','了','吗','啊','拉','呢','请问','啥']\n",
    "\n",
    "def word_segment(text):\n",
    "    seg_list = jieba.cut(text)\n",
    "    word_list = []\n",
    "    for word in seg_list:\n",
    "        if word not in stop_words:\n",
    "            word_list.append(word)\n",
    "    line = \" \".join(word_list)\n",
    "    return line\n",
    "\n",
    "\n",
    "#2: 文档问答。1:闲聊\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(X_train)):\n",
    "        f.write('__label__'+str(y_train.values[i])+'\\t'+word_segment(X_train.values[i])+'\\n')\n",
    "        \n",
    "f.close()\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(X_test)):\n",
    "        f.write('__label__'+str(y_test.values[i])+'\\t'+word_segment(X_test.values[i])+'\\n')\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a689f-9008-4263-93d3-d461a937df8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 帮我订机票\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dm client received: 请输入您的出发地\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 深圳\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dm client received: 请输入您的目的地\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 北京\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dm client received: 情输入出发时间\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 明晚八点\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dm client received: 输入非法，请重新输入\n"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "from protos.nlp.dm import dm_pb2\n",
    "from protos.nlp.dm import dm_pb2_grpc\n",
    "\n",
    "def run():\n",
    "    # 连接 rpc 服务器\n",
    "    channel = grpc.insecure_channel('localhost:50051')\n",
    "    # 调用 rpc 服务\n",
    "    stub = dm_pb2_grpc.DmStub(channel)\n",
    "    history = {}\n",
    "    while True:\n",
    "        text = input()\n",
    "        response = stub.GetDm(dm_pb2.DmRequest(text=text, history=history,session_id='1', robot_id = -1))\n",
    "        print(\"Dm client received: \" + response.answer)\n",
    "        history = response.history\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683006f0-6b4e-435f-86dc-2ab62325aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75d26ac-2aa3-49e1-a7db-15bc361bc8ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/0p/cjvwlkfs59d5t070gww_cp300000gn/T/jieba.cache\n",
      "Loading model cost 0.276 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "2023-03-08 10:31:04,518 log.log INFO query input:帮订一张从深圳到北京的机票\n",
      "2023-03-08 10:31:04,518 log.log INFO query rewrite output:帮订一张从深圳到北京的机票\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot_type:  \n",
      "chat_repo:  1\n",
      "skill_type:  DH\n",
      "FAQ:  False\n",
      "帮订一张从深圳到北京的机票\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 10:31:04,746 log.log INFO predict class:Mutil-Turn\n",
      "2023-03-08 10:31:04,747 log.log INFO slots :['start_city', 'end_city', 'time'] \n",
      "\n",
      "2023-03-08 10:31:04,747 log.log INFO shot entities :['深圳', '北京'] \n",
      "\n",
      "2023-03-08 10:31:04,748 log.log INFO current task id :[0] \n",
      "\n",
      "2023-03-08 10:31:04,748 log.log INFO score :1.0\n",
      "2023-03-08 10:31:04,749 log.log INFO dm response :情输入出发时间 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情输入出发时间\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse \n",
    "sys.path.append('./DM/')\n",
    "sys.path.append('./DM/models/')\n",
    "from DmClass import *\n",
    "\n",
    "text = '帮订一张从深圳到北京的机票'\n",
    "history = {'conversation':['','']}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--robot_type', default='', choices=['DH', 'KF'], type=str, help='Robot type, DH or KF')\n",
    "parser.add_argument('--chat_repo', default=1, choices=[0, 1], type=int, help='which repo used by chat_infer module')\n",
    "parser.add_argaument('--skill_type', default='DH', choices=['DH', 'KF'], help='What type of multi-turn capability the robot has')\n",
    "parser.add_argument('--FAQ', action=\"store_true\", default=False, help='if use the faq server')\n",
    "parser.add_argument('--QR', action=\"store_true\", default=False, help='if use the query rewrite server')\n",
    "\n",
    "parser.add_argument('--qr_ip', default='192.168.68.26', type=str)\n",
    "parser.add_argument('--qr_port', default='60005', type=str)\n",
    "parser.add_argument('--faq_ip', default='172.31.208.10', type=str)\n",
    "parser.add_argument('--faq_port', default='58999', type=str)\n",
    "parser.add_argument('--docqa_ip', default='172.31.208.4', type=str)\n",
    "parser.add_argument('--docqa_port', default='60005', type=str)\n",
    "parser.add_argument('--chat_ip', default='172.31.208.9', type=str)\n",
    "parser.add_argument('--chat_port', default='60005', type=str)\n",
    "parser.add_argument('--emotion_ip', default='172.31.208.10', type=str)\n",
    "parser.add_argument('--emotion_port', default='58999', type=str)\n",
    "args = parser.parse_args(args=[])\n",
    "dm  = DmClass(args)\n",
    "dm.load_data(text, history, '1', -1)\n",
    "\n",
    "ans, history, emotion = dm.response()\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57f648-0131-4ff3-bfcd-f25292f3568b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 帮我订机票\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 帮我订机票\n",
      "Dm client received: 你要去哪里? \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 深圳\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 深圳\n",
      "Dm client received: 去干嘛? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DM1.1\n",
    "#闲聊、Faq、文档问答（wiki、mita）\n",
    "import grpc\n",
    "from protos.nlp.dm import dm_pb2\n",
    "from protos.nlp.dm import dm_pb2_grpc\n",
    "\n",
    "def run():\n",
    "    # 连接 rpc 服务器\n",
    "    channel = grpc.insecure_channel('172.31.208.4:55555')\n",
    "    # 调用 rpc 服务\n",
    "    stub = dm_pb2_grpc.DmStub(channel)\n",
    "    \n",
    "    #第一轮\n",
    "    history = {}\n",
    "    while True:\n",
    "        text = input()\n",
    "        print('text:',text)\n",
    "        response = stub.GetDm(dm_pb2.DmRequest(text=text, history=history, session_id='1', robot_id = -1))\n",
    "        print(\"Dm client received: \" + response.answer,'\\n')\n",
    "        history = response.history\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f1fcba-a1b9-4776-9379-7dc9da0a0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_1 = pd.read_csv('./faq.txt', sep='\\t', header=None, names=['query', 'faq', 'answer','tts'], usecols = ['query'])\n",
    "chat = data_1['query'].tolist()\n",
    "\n",
    "with open('tmp.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(chat)):\n",
    "        f.write(str(chat[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dcf4447-8c20-4c96-965e-4816cfed055c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\xba'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastText\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model.save('models/model.bin')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = FastText.load('models/model.bin')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mFastText\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/model.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/gensim/models/fasttext.py:637\u001b[0m, in \u001b[0;36mFastText.load\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    619\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a previously saved `FastText` model.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrethrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/gensim/models/word2vec.py:1942\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \n\u001b[1;32m   1925\u001b[0m \u001b[38;5;124;03mSee Also\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \n\u001b[1;32m   1940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWord2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Word2Vec):\n\u001b[1;32m   1944\u001b[0m         rethrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/gensim/utils.py:486\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    482\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[1;32m    484\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[0;32m--> 486\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[1;32m    488\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/gensim/utils.py:1461\u001b[0m, in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \n\u001b[1;32m   1449\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \n\u001b[1;32m   1459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xba'."
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# model.save('models/model.bin')\n",
    "# model = FastText.load('models/model.bin')\n",
    "FastText.load('models/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923320c-11f9-4516-9b55-ca664e6bab3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
