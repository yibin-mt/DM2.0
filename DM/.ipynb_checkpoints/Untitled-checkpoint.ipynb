{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738ec7b-2279-40ed-9b41-74862022f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAQ\n",
    "import grpc\n",
    "from protos.nlp.faq import faq_pb2 as pb2\n",
    "from protos.nlp.faq import faq_pb2_grpc as pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "class Faq_inference():\n",
    "    def __init__(self):\n",
    "        super(Faq_inference, self).__init__()\n",
    "\n",
    "    def faq_answer(self, query):\n",
    "        # conn = grpc.insecure_channel('192.168.4.29:60009')\n",
    "        conn = grpc.insecure_channel('172.31.208.10:58999')\n",
    "        # conn = grpc.insecure_channel('172.31.208.10:58999')\n",
    "        client = pb2_grpc.FaqStub(conn)\n",
    "        res = client.GetFaq(pb2.FaqRequest(query=query, robot_id=1), timeout=1)\n",
    "        answer = res.answer #faq对应的答案\n",
    "        match = res.match   #匹配到的相似问句\n",
    "        faq = res.faq       #对应的标准问句\n",
    "        score = res.score   #匹配模型匹配的分数，可以视为置信度\n",
    "        return answer, faq, score, match\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"我是你爸爸\"\n",
    "    tt = Faq_inference()\n",
    "    print(tt.faq_answer(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb5bdc-a5fe-44a0-8811-11a425e7cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 意图识别\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "# 读取csv文件(前n行)\n",
    "def read(train_set_path, test_set_path):\n",
    "    data_1 = pd.read_csv('./faq.txt', sep='\\t', header=None, names=['query', 'faq', 'answer','tts'], usecols = ['query'])\n",
    "    data_1['class'] = 0\n",
    "    data_temp = pd.read_csv('qingyun_chat.txt', header=None, names=['query'])\n",
    "    data_temp['class'] = 1\n",
    "    data_2 = pd.read_csv('./chat.txt', sep='\\t', header=None, names=['match', 'query', 'answer','tts'], usecols = ['query'])\n",
    "    data_2['class'] = 1\n",
    "    data_3 = pd.read_csv('docqa.csv', header=None, names = ['query'])\n",
    "    data_3['class'] = 2\n",
    "    data_1 = data_1.drop_duplicates()\n",
    "    data_2 = data_2.drop_duplicates()\n",
    "    data = pd.concat((data_1, data_2, data_3, data_temp))\n",
    "    \n",
    "    data['query'] = data['query'].apply(lambda x: ' '.join(str(x)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['query'], data['class'], test_size=0.15)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 训练拟合模型\n",
    "def train(X_train, y_train):\n",
    "    vectorizer = TfidfVectorizer(max_features=800000, min_df=2, analyzer='word', token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", ngram_range=(1, 5)) \n",
    "    x_train = vectorizer.fit_transform(X_train)    \n",
    "    # 构建逻辑回归模型\n",
    "    logistic_model = LogisticRegression(C=3, dual=False) \n",
    "    # 拟合,训练\n",
    "    logistic_model.fit(x_train, y_train) \n",
    "    \n",
    "    tfidftransformer_path = 'models/tfidftransformer.pkl'\n",
    "    with open(tfidftransformer_path, 'wb') as fw:\n",
    "         pickle.dump(vectorizer, fw)        \n",
    "    joblib.dump(logistic_model, \"models/logisitic_model.pkl\") \n",
    "    return vectorizer, logistic_model\n",
    "\n",
    "\n",
    "# 保存预测结果\n",
    "def save(test_data, y_test, save_path):\n",
    "    # 保存结果\n",
    "    y_test = y_test.tolist()  # 把(n,)的一维矩阵y_test转化为矩阵列表形式存储\n",
    "    test_data['class'] = y_test + 1  # 新建一个class列,存入类别值(类别还原+1)\n",
    "    data_result = test_data.loc[:, ['id', 'class']]  # 根据index索引所有行和id,class列\n",
    "    data_result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    chars = '*' * 20\n",
    "    '''1.读取数据'''\n",
    "    # old_time = time.time() \n",
    "    X_train, X_test, y_train, y_test = read('./faq.txt', './chat.txt')\n",
    "    # current_time = time.time()\n",
    "    # print(chars + '读取完成!用时{}s'.format(round(current_time-old_time,2)) + chars + '\\n')\n",
    "\n",
    "    '''2.训练'''\n",
    "    vectorizer, logistic_model = train(X_train, y_train)\n",
    "    \n",
    "    vectorizer = pickle.load(open('models/tfidftransformer.pkl', \"rb\"))\n",
    "    logistic_model = joblib.load('models/logisitic_model.pkl')\n",
    "    \n",
    "    '''3.预测'''\n",
    "    x_test = vectorizer.transform(X_test)\n",
    "    y_pred = logistic_model.predict(x_test) # (n,)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(acc_score)\n",
    "    F1_score = f1_score(y_test, y_pred, average='macro')\n",
    "    print(F1_score)\n",
    "    \n",
    "    \n",
    "    #0:闲聊 ， 1:客服， 2: 文档问答（知识问答）\n",
    "    C2 = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "    # 计算百分比\n",
    "    C2_normal = np.round(C2/np.sum(C2, axis=1).reshape(-1, 1), 2)\n",
    "\n",
    "    sns.set()\n",
    "    f, ax = plt.subplots()\n",
    "    sns.heatmap(C2_normal, cmap=\"YlGnBu\", annot=True, ax=ax) # heatmap\n",
    "\n",
    "    #ax.set_title('Normalized confusion maxtrix') # title\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.xaxis.set_ticklabels([0, 1, 2])\n",
    "    ax.yaxis.set_ticklabels([0, 1, 2])\n",
    "    \n",
    "    print('Over!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e295b-1bb5-42cb-9a92-0a6d20d04015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping = {-1:'Unknown', 0:'Faq', 1:'Chat', 2:'DocQA', 3:'Mutil-Turn'}\n",
    "import pickle\n",
    "import joblib\n",
    "query = '她的妻子叫什么'\n",
    "vectorizer = pickle.load(open('models/tfidftransformer.pkl', \"rb\"))\n",
    "model = joblib.load('models/logisitic_model.pkl')\n",
    "pred_class = model.predict(vectorizer.transform([' '.join(query)]))\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05e1d4-fa53-4eae-a2d9-820e132fd871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from protos.nlp.chat import chat_pb2\n",
    "from protos.nlp.chat import chat_pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "\n",
    "\n",
    "class Chat_inference(object):\n",
    "    def __init__(self):\n",
    "        super(Chat_inference, self).__init__()\n",
    "\n",
    "    def response(self, history_utts, query):\n",
    "        conn = implementations.insecure_channel(\"172.31.208.9\", 60005)\n",
    "        client = chat_pb2_grpc.ChatStub(channel=conn._channel)\n",
    "        res = client.GetChat(chat_pb2.ChatRequest(history=history_utts, query=query, session_id='1', robot_id=1), timeout=60)\n",
    "        answer = res.answer\n",
    "        score = res.score\n",
    "        query = res.query\n",
    "        sessionid = res.session_id\n",
    "        return answer, score, query, sessionid\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_infer = Chat_inference()\n",
    "    \n",
    "    history_utts = [{\"conversation\": [],'intent':[],'slots':[],'entities':[],'task_id':[]}]\n",
    "    query = \"你好\"\n",
    "    start_time = time.time()\n",
    "    results = chat_infer.response(history_utts, query)\n",
    "    end_time = time.time()\n",
    "    print('*' * 20 + '回复完成!用时{:.2f}s'.format(end_time - start_time) + '*' * 20)\n",
    "    print('results is:', *results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24aeb4-4d3d-4015-888e-71247deecfca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DocQa\n",
    "from protos.nlp.docqa import docqa_pb2\n",
    "from protos.nlp.docqa import docqa_pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "\n",
    "class DocQAInference(object):\n",
    "    def __init__(self, config):\n",
    "        self.host = config.get('host', \"172.31.208.4\")\n",
    "        self.port = config.get('port', 60005)\n",
    "        # self.port = config.get('port', 6082)\n",
    "        self.timeout = config.get('timeout', 60)\n",
    "\n",
    "    def docqa(self, index_id, query):\n",
    "        conn = implementations.insecure_channel(self.host, self.port)\n",
    "        client = docqa_pb2_grpc.DocqaStub(channel=conn._channel)\n",
    "        res = client.GetDocqa(docqa_pb2.DocqaRequest(index_id=index_id, query=query, session_id='1', robot_id=1), timeout=self.timeout)\n",
    "        return res\n",
    "\n",
    "#‘诗句：‘夜来风雨声’的下一句是什么’\n",
    "if __name__ == \"__main__\":\n",
    "    query_index_list = [\n",
    "        (\"中国有多少个省\", \"wiki\"),\n",
    "    ]\n",
    "    query_list = [item[0] for item in query_index_list]\n",
    "    index_list = [item[1] for item in query_index_list]\n",
    "    # index_list = index_list*25\n",
    "    # query_list = query_list*25\n",
    "    \n",
    "    inference = DocQAInference({})\n",
    "    inference.docqa(index_list[0], query_list[0])\n",
    "\n",
    "    import numpy as np\n",
    "    import time\n",
    "    query_lens = []\n",
    "    latency_list = []\n",
    "    for index, query in zip(index_list, query_list):\n",
    "        print(f\"index={index}\")\n",
    "        print(f\"query={query}\")\n",
    "        start_time = time.time()    \n",
    "        docqa_rst = inference.docqa(index, query)\n",
    "        time_cost = time.time() - start_time\n",
    "        print(f\"result={MessageToJson(docqa_rst, ensure_ascii=False, indent=2)}\")\n",
    "        latency_list.append(time_cost*1000)\n",
    "        query_len = len(query)\n",
    "        query_lens.append(query_len)\n",
    "        \n",
    "                \n",
    "    print(f\"mean of query len is {np.mean(query_lens)}\")\n",
    "    print(f\"mean of latency is {np.mean(latency_list)}ms\")\n",
    "    print(f\"0.95 of latency is {np.quantile(latency_list, 0.95)}ms\")\n",
    "    print(f\"0.99 of latency is {np.quantile(latency_list, 0.99)}ms\")\n",
    "    \n",
    "#特朗普的弟弟叫什么\n",
    "#碧昂斯的出生地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe830df7-fd27-4e2e-88db-da678ef37c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"squad_data.json\", encoding='utf-8', errors='ignore') as json_data:\n",
    "     data = json.load(json_data, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5717-5efc-48d4-a0f5-47a858540adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a46c2-f5c7-41d7-968c-ce1bdec2f164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = []\n",
    "for i in range(len(data['data'])):\n",
    "    for j in range(len(data['data'][i]['paragraphs'])):\n",
    "        for z in data['data'][i]['paragraphs'][j]['qas']:\n",
    "            query.append(z['question'])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1bc91-f5b5-4a39-ba57-c45f1947fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca732f5-a0cb-42bc-ad5a-dc6564b7ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('docqa_1.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034ee3e-7b79-42fa-92d9-3877a0006279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#改写模块\n",
    "import grpc\n",
    "import sys\n",
    "from protos.nlp.rewrite import rewrite_pb2\n",
    "from protos.nlp.rewrite import rewrite_pb2_grpc\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "from grpc.beta import implementations\n",
    "\n",
    "class IUR_inference(object):\n",
    "    def __init__(self, config):\n",
    "        self.host = config.get('host', \"192.168.68.26\")\n",
    "        self.port = config.get('port', 60005)\n",
    "        self.timeout = config.get('timeout', 60)\n",
    "\n",
    "    def iur(self, history_utts, query):\n",
    "        conn = implementations.insecure_channel(self.host, self.port)\n",
    "        client = rewrite_pb2_grpc.ReWriteStub(channel=conn._channel)\n",
    "        res = client.GetReWrite(rewrite_pb2.ReWriteRequest(history=history_utts, query=query, session_id='1', robot_id=1), timeout=self.timeout)\n",
    "        new_query = res.new_query\n",
    "        score = res.score\n",
    "        return new_query, score\n",
    "\n",
    "    def query_write_strategy(self, query, conversation):\n",
    "        new_query_1, qr_score_1 = self.iur(conversation[:2], query)\n",
    "        new_query_2, qr_score_2 = self.iur(conversation[2:], query)\n",
    "        new_query_3, qr_score_3 = self.iur(conversation, query)\n",
    "        new_query, qr_score = new_query_1, qr_score_1\n",
    "        if qr_score_1 <qr_score_2:\n",
    "            new_query = new_query_2\n",
    "            qr_score = qr_score_2\n",
    "        print(new_query_1, qr_score_1)\n",
    "        print(new_query_2, qr_score_2)\n",
    "        print(new_query_3, qr_score_3)\n",
    "        if qr_score>0.8 and new_query!=query:\n",
    "            return new_query\n",
    "        return query\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # history_utts = [\"\", \"\", \"北京可以办理工作居住证么？\", \"可以，根据政府政策规定，入职满一年后满足条件的可办理。\"]\n",
    "    # query = \"多久能下来？\"\n",
    "    # history_utts = [\"\", \"\", \"有松下的吹风机吗\", \"没有哦亲\"]\n",
    "    # query = \"那有戴森的吗\"\n",
    "    # history_utts = [\"\", \"\", \"帮我放周杰伦的歌\", \"好的，正在播放周杰伦的稻香\"]\n",
    "    # query = \"算了，不要放他的，放张学友的\"\n",
    "    # history_utts = ['','']\n",
    "    # history_utts = ['你知道碧昂斯吗', '肯定知道啊','那你知道她的第一张专辑吗','危险爱情']\n",
    "    # history_utts = ['你知道碧昂斯吗','我的朋友有很多啊，你也是我朋友啊你忘啦？', '她的第一张专辑是什么', '危险爱情']\n",
    "    history_utts = ['','','今天的天气怎么样', '阴天,有点闷热']\n",
    "    # history_utts = ['怎么申请设备呢', 'IT标准库存资产在金蝶系统-人人资产填写申请；IT标准配件在飞书-工作台-审批填写耗材领用流程；非标准库存配件在金蝶系统填写采购申请。', '那北京可以办理居住证吗', '可以，根据政府政策规定，入职满一年后满足条件的可办理。']\n",
    "    # history_utts = ['怎么申请设备', 'IT标准库存资产在金蝶系统-人人资产填写申请；IT标准配件在飞书-工作台-审批填写耗材领用流程；非标准库存配件在金蝶系统填写采购申请。', '北京可以办理居住证吗', '可以，需上家公司减员后绑定公司关联码后系统提交材料即可。']\n",
    "    # history_utts = ['你好', '你好呀，我可以帮你做点啥嘛', '你认识碧昂斯吗', '那就多了去了，亲人朋友名人数不胜数呐！']\n",
    "    # query=\"她的第一张专辑是什么\"\n",
    "    query = '明天的呢'\n",
    "    tt = IUR_inference({})\n",
    "    new_query = tt.query_write_strategy(query, history_utts)\n",
    "    # new_query, score = tt.iur(history_utts, query)\n",
    "    print(new_query)\n",
    "#0.9014959335327148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a32171-dca4-4f2a-b351-8fcf9082a506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import jieba.posseg as psg\n",
    "import jieba\n",
    "jieba.add_word('今天', 100000)\n",
    "jieba.add_word('明天', 100000)\n",
    "jieba.add_word('后天', 100000)\n",
    "jieba.add_word('上午',100000)\n",
    "jieba.add_word('下午',100000)\n",
    "\n",
    "def get_time_word(text):\n",
    "    time_res = []\n",
    "    word = ''\n",
    "    keyDate = {'今天': 0, '明天':1, '后天': 2}\n",
    "    for k, v in psg.cut(text):\n",
    "        if k in keyDate:\n",
    "            if word != '':\n",
    "                time_res.append(word)\n",
    "            time_res.append(k)\n",
    "        elif word != '':\n",
    "            if v in ['m', 't']:\n",
    "                word = word + k\n",
    "            else:\n",
    "                time_res.append(word)\n",
    "                word = ''\n",
    "        elif v in ['m', 't'] and '张' not in k:\n",
    "            word = k\n",
    "    if word != '':\n",
    "        time_res.append(word)\n",
    "    return time_res\n",
    "\n",
    "def time_extract(text):\n",
    "    # text = text\n",
    "    time_res = []\n",
    "    word = ''\n",
    "    keyDate = {'今天': 0, '明天':1, '后天': 2}\n",
    "    for k, v in psg.cut(text):\n",
    "        if k in keyDate:\n",
    "            if word != '':\n",
    "                time_res.append(word)\n",
    "            word = (datetime.today() + timedelta(days=keyDate.get(k, 0))).strftime('%Y年%m月%d日')\n",
    "        elif word != '':\n",
    "            if v in ['m', 't']:\n",
    "                word = word + k\n",
    "            else:\n",
    "                time_res.append(word)\n",
    "                word = ''\n",
    "        elif v in ['m', 't'] and '张' not in k:\n",
    "            word = k\n",
    "    if word != '':\n",
    "        time_res.append(word)\n",
    "    result = list(filter(lambda x: x is not None, [check_time_valid(w) for w in time_res]))\n",
    "    final_res = [parse_datetime(w) for w in result]\n",
    "    return [x for x in final_res if x is not None]\n",
    "\n",
    "def check_time_valid(word):\n",
    "    m = re.match(\"\\d+$\", word)\n",
    "    if m:\n",
    "        if len(word) <= 6:\n",
    "            return None\n",
    "    word1 = re.sub('[号|日]\\d+$', '日', word)\n",
    "    if word1 != word:\n",
    "        return check_time_valid(word1)\n",
    "    else:\n",
    "        return word1\n",
    "    \n",
    "def parse_datetime(msg):\n",
    "    if msg is None or len(msg) == 0:\n",
    "        return None\n",
    "\n",
    "    if '午' not in msg and '点' not in msg:\n",
    "        dt = parse(msg, fuzzy=True)\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        m = re.match(\n",
    "            r\"([0-9零一二两三四五六七八九十]+年)?([0-9一二两三四五六七八九十]+月)?([0-9一二两三四五六七八九十]+[号日])?([上中下午晚早]+)?([0-9零一二两三四五六七八九十百]+[点:\\.时])?([0-9零一二三四五六七八九十百]+分?)?([0-9零一二三四五六七八九十百]+秒)?\",\n",
    "            msg)\n",
    "        if m.group(0) is not None:\n",
    "            res = {\n",
    "                \"year\": m.group(1),\n",
    "                \"month\": m.group(2),\n",
    "                \"day\": m.group(3),\n",
    "                \"hour\": m.group(5) if m.group(5) is not None else '00',\n",
    "                \"minute\": m.group(6) if m.group(6) is not None else '00',\n",
    "                \"second\": m.group(7) if m.group(7) is not None else '00',\n",
    "            }\n",
    "            params = {}\n",
    "\n",
    "            for name in res:\n",
    "                if res[name] is not None and len(res[name]) != 0:\n",
    "                    tmp = None\n",
    "                    if name == 'year':\n",
    "                        tmp = year2dig(res[name][:-1])\n",
    "                    else:\n",
    "                        tmp = cn2dig(res[name][:-1])\n",
    "                    if tmp is not None:\n",
    "                        params[name] = int(tmp)\n",
    "            target_date = datetime.today().replace(**params)\n",
    "            is_pm = m.group(4)\n",
    "            if is_pm is not None:\n",
    "                if is_pm == u'下午' or is_pm == u'晚上' or is_pm =='中午':\n",
    "                    hour = target_date.time().hour\n",
    "                    if hour < 12:\n",
    "                        target_date = target_date.replace(hour=hour + 12)\n",
    "            return target_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "UTIL_CN_NUM = {\n",
    "    '零': 0, '一': 1, '二': 2, '两': 2, '三': 3, '四': 4,\n",
    "    '五': 5, '六': 6, '七': 7, '八': 8, '九': 9,\n",
    "    '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n",
    "    '5': 5, '6': 6, '7': 7, '8': 8, '9': 9\n",
    "}\n",
    "UTIL_CN_UNIT = {'十': 10, '百': 100, '千': 1000, '万': 10000}\n",
    "def cn2dig(src):\n",
    "    if src == \"\":\n",
    "        return None\n",
    "    m = re.match(\"\\d+\", src)\n",
    "    if m:\n",
    "        return int(m.group(0))\n",
    "    rsl = 0\n",
    "    unit = 1\n",
    "    for item in src[::-1]:\n",
    "        if item in UTIL_CN_UNIT.keys():\n",
    "            unit = UTIL_CN_UNIT[item]\n",
    "        elif item in UTIL_CN_NUM.keys():\n",
    "            num = UTIL_CN_NUM[item]\n",
    "            rsl += num * unit\n",
    "        else:\n",
    "            return None\n",
    "    if rsl < unit:\n",
    "        rsl += unit\n",
    "    return rsl\n",
    "def year2dig(year):\n",
    "    res = ''\n",
    "    for item in year:\n",
    "        if item in UTIL_CN_NUM.keys():\n",
    "            res = res + str(UTIL_CN_NUM[item])\n",
    "        else:\n",
    "            res = res + item\n",
    "    m = re.match(\"\\d+\", res)\n",
    "    if m:\n",
    "        if len(m.group(0)) == 2:\n",
    "            return int(datetime.datetime.today().year/100)*100 + int(m.group(0))\n",
    "        else:\n",
    "            return int(m.group(0))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a037a0-6596-44da-8bc1-5b79f694c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def ner(query):\n",
    "    entities = []\n",
    "    slots = []\n",
    "    query = query.replace('今晚','今天晚上').replace('明早','明天早上').replace('明晚','明天晚上')\n",
    "\n",
    "    time = time_extract(query)\n",
    "    if time:\n",
    "        slots.append('time')\n",
    "        entities.append(time[0])\n",
    "    time_word = get_time_word(query)\n",
    "    for word in time_word:\n",
    "        query = query.replace(word, '')\n",
    "    print(query)\n",
    "\n",
    "    m = re.findall(r\"订.*?[0-9一二两三四五六七八九十]?张?从?([\\u4e00-\\u9fa5]{2,5}?)?(出发)?[到｜飞|去｜]([\\u4e00-\\u9fa5]{2,5}?)的\",query)\n",
    "    if m:\n",
    "        start_city = m[0][0]\n",
    "        end_city = m[0][2]\n",
    "        if start_city:\n",
    "            slots.append('start_city')\n",
    "            entities.append(start_city)\n",
    "        if end_city:\n",
    "            slots.append('end_city')\n",
    "            entities.append(end_city)\n",
    "\n",
    "        print(start_city)\n",
    "        print(end_city)\n",
    "    else:\n",
    "        pass\n",
    "    return slots, entities\n",
    "    \n",
    "ner('明晚八点')\n",
    "# ner('订一张明晚八点的机票')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27090e6-c728-4693-b418-cce61c6c14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '帮我订一张明晚八点上海到深圳的机票'\n",
    "print(text1, time_extract(text1), sep=':')\n",
    "\n",
    "# text2 = '预定28号的房间'\n",
    "# print(text2, time_extract(text2), sep=':')\n",
    "\n",
    "# text3 = '我要从26号下午4点住到11月2号'\n",
    "# print(text3, time_extract(text3), sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc8c6f-2914-432d-912c-d16499502b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#情绪识别\n",
    "\n",
    "import grpc\n",
    "from protos.nlp.faq import faq_pb2, faq_pb2_grpc\n",
    "from protos.nlp.emotion import emotion_pb2, emotion_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('172.31.208.10:58999')\n",
    "texts = ['你真棒']\n",
    "\n",
    "# emotion\n",
    "emotion_client = emotion_pb2_grpc.EmotionStub(channel)\n",
    "for query in texts:\n",
    "    resp = emotion_client.GetEmotion(emotion_pb2.EmotionRequest(query=query, session_id='1', robot_id=2))\n",
    "    # print(resp.answer.happy, resp.answer.sad, resp.answer.angry, resp.answer.disgust,\n",
    "    #       resp.answer.fear, resp.answer.surprise, resp.answer.none)\n",
    "    print(resp)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01108689-0d3d-4c6e-b3b2-9e0182e6c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  57491\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2813381 lr:  0.000000 avg.loss:  0.068367 ETA:   0h 0m 0s\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17536, 0.9757641423357665, 0.9757641423357665)\n",
      "世界 最高 山峰 是 哪座\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#领域分类模型（Fasttext）\n",
    "import fasttext\n",
    "import jieba\n",
    "# from gensim.models import FastText\n",
    "jieba.add_word('你知道',100000)\n",
    "\n",
    "model = fasttext.train_supervised(input = r\"./train.txt\", label_prefix=\"__label__\", epoch=10, verbose=2, lr=0.04, loss=\"softmax\")\n",
    "\n",
    "model.save_model(r\"models/model.bin\")\n",
    "\n",
    "stop_words = ['的','了','吗','啊','拉','呢','你知道','告诉', '请问']\n",
    "\n",
    "def word_segment(text):\n",
    "    seg_list = jieba.cut(text)\n",
    "    word_list = []\n",
    "    for word in seg_list:\n",
    "        if word not in stop_words:\n",
    "            word_list.append(word)\n",
    "    line = \" \".join(word_list)\n",
    "    print(line)\n",
    "    return line\n",
    "\n",
    "# 预测\n",
    "model = fasttext.load_model(r\"models/model.bin\")\n",
    "print(model.test(r\"./test.txt\")  )\n",
    "\n",
    "# text = 'hi穆莎'\n",
    "text = '世界最高的山峰是哪座'\n",
    "print(model.predict(word_segment(text))[0][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943ca756-1fa3-48f1-b2b5-ac605ecc9960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/0p/cjvwlkfs59d5t070gww_cp300000gn/T/jieba.cache\n",
      "Loading model cost 0.281 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#文件预处理\n",
    "import jieba\n",
    "jieba.add_word('你知道',100000)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Faq\n",
    "data_1 = pd.read_csv('./faq.txt', sep='\\t', header=None, names=['query'])\n",
    "data_1['class'] = 0\n",
    "\n",
    "#闲聊\n",
    "data_temp = pd.read_csv('qingyun_chat.txt', header=None, names=['query'])[:50000]\n",
    "data_temp['class'] = 1\n",
    "data_2 = pd.read_csv('./chat.txt', sep='\\t', header=None, names=['query'])\n",
    "data_2['class'] = 1\n",
    "\n",
    "#文档问答\n",
    "data_3 = pd.read_csv('docqa_2.csv', header=None, names = ['query'])\n",
    "data_3['class'] = 2\n",
    "\n",
    "data_2 = data_2.drop_duplicates()\n",
    "data_temp = data_temp.drop_duplicates()\n",
    "data = pd.concat((data_1, data_2, data_3, data_temp))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['query'], data['class'], test_size=0.15)\n",
    "\n",
    "stop_words = ['你知道','的','了','吗','啊','拉','呢','请问','啥']\n",
    "\n",
    "def word_segment(text):\n",
    "    seg_list = jieba.cut(text)\n",
    "    word_list = []\n",
    "    for word in seg_list:\n",
    "        if word not in stop_words:\n",
    "            word_list.append(word)\n",
    "    line = \" \".join(word_list)\n",
    "    return line\n",
    "\n",
    "\n",
    "#2: 文档问答。1:闲聊\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(X_train)):\n",
    "        f.write('__label__'+str(y_train.values[i])+'\\t'+word_segment(X_train.values[i])+'\\n')\n",
    "        \n",
    "f.close()\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(X_test)):\n",
    "        f.write('__label__'+str(y_test.values[i])+'\\t'+word_segment(X_test.values[i])+'\\n')\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d26ac-2aa3-49e1-a7db-15bc361bc8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse \n",
    "sys.path.append('./DM/')\n",
    "sys.path.append('./DM/models/')\n",
    "from DmClass import *\n",
    "\n",
    "text = '帮订一张从深圳到北京的机票'\n",
    "history = {'conversation':['','']}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--robot_type', default='', choices=['DH', 'KF'], type=str, help='Robot type, DH or KF')\n",
    "parser.add_argument('--chat_repo', default=1, choices=[0, 1], type=int, help='which repo used by chat_infer module')\n",
    "parser.add_argaument('--skill_type', default='DH', choices=['DH', 'KF'], help='What type of multi-turn capability the robot has')\n",
    "parser.add_argument('--FAQ', action=\"store_true\", default=False, help='if use the faq server')\n",
    "parser.add_argument('--QR', action=\"store_true\", default=False, help='if use the query rewrite server')\n",
    "\n",
    "parser.add_argument('--qr_ip', default='192.168.68.26', type=str)\n",
    "parser.add_argument('--qr_port', default='60005', type=str)\n",
    "parser.add_argument('--faq_ip', default='172.31.208.10', type=str)\n",
    "parser.add_argument('--faq_port', default='58999', type=str)\n",
    "parser.add_argument('--docqa_ip', default='172.31.208.4', type=str)\n",
    "parser.add_argument('--docqa_port', default='60005', type=str)\n",
    "parser.add_argument('--chat_ip', default='172.31.208.9', type=str)\n",
    "parser.add_argument('--chat_port', default='60005', type=str)\n",
    "parser.add_argument('--emotion_ip', default='172.31.208.10', type=str)\n",
    "parser.add_argument('--emotion_port', default='58999', type=str)\n",
    "args = parser.parse_args(args=[])\n",
    "dm  = DmClass(args)\n",
    "dm.load_data(text, history, '1', -1)\n",
    "\n",
    "ans, history, emotion = dm.response()\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd57f648-0131-4ff3-bfcd-f25292f3568b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['介绍一下深圳']\n"
     ]
    },
    {
     "ename": "_MultiThreadedRendezvous",
     "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:Failed to pick subchannel {created_time:\"2023-08-07T11:53:42.687837232+08:00\", children:[UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused {created_time:\"2023-08-07T11:53:42.687833524+08:00\", grpc_status:14}]}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m i\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<n>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m chatglm \u001b[38;5;241m=\u001b[39m ChatGlm_inference()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m chatglm\u001b[38;5;241m.\u001b[39mresponse([], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m介绍一下深圳\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mChatGlm_inference.response\u001b[0;34m(self, hisorty, query)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[1;32m     19\u001b[0m res \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mNLGGet(nlg_pb2\u001b[38;5;241m.\u001b[39mNLGMessage(context\u001b[38;5;241m=\u001b[39mmessage, prompt_prefix\u001b[38;5;241m=\u001b[39msystem))\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m i\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<n>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/grpc/_channel.py:426\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/grpc/_channel.py:826\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:Failed to pick subchannel {created_time:\"2023-08-07T11:53:42.687837232+08:00\", children:[UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused {created_time:\"2023-08-07T11:53:42.687833524+08:00\", grpc_status:14}]}\"\n>"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import sys\n",
    "sys.path.append('chatglm_protos/')\n",
    "import nlg_pb2\n",
    "import nlg_pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "\n",
    "class ChatGlm_inference(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ChatGlm_inference, self).__init__()\n",
    "\n",
    "    def response(self, hisorty, query):\n",
    "        conn = implementations.insecure_channel(\"192.168.68.26\",60005)\n",
    "        client = nlg_pb2_grpc.NLGServiceStub(channel=conn._channel)\n",
    "        system = '你是摩尔线程的对话数字人，名字叫做穆莎。你所在的空间，是真实存在于摩尔北京总部的一个展厅，它是通过混合渲染管线制作和渲染的。接下来的问题，回答尽量不超过90个字。'\n",
    "        message = hisorty+[query]\n",
    "        print(message)\n",
    "        res = client.NLGGet(nlg_pb2.NLGMessage(context=message, prompt_prefix=system))\n",
    "        for i in res:\n",
    "            yield i.output.replace('<n>', '\\n')\n",
    "            \n",
    "chatglm = ChatGlm_inference()\n",
    "for answer in chatglm.response([], '介绍一下深圳'):\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3d785-6578-4167-b036-02e46f756d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
