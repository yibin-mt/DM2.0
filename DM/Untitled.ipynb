{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738ec7b-2279-40ed-9b41-74862022f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAQ\n",
    "import grpc\n",
    "from protos.nlp.faq import faq_pb2 as pb2\n",
    "from protos.nlp.faq import faq_pb2_grpc as pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "class Faq_inference():\n",
    "    def __init__(self):\n",
    "        super(Faq_inference, self).__init__()\n",
    "\n",
    "    def faq_answer(self, query):\n",
    "        # conn = grpc.insecure_channel('192.168.4.29:60009')\n",
    "        conn = grpc.insecure_channel('172.31.208.10:58999')\n",
    "        # conn = grpc.insecure_channel('172.31.208.10:58999')\n",
    "        client = pb2_grpc.FaqStub(conn)\n",
    "        res = client.GetFaq(pb2.FaqRequest(query=query, robot_id=1), timeout=1)\n",
    "        answer = res.answer #faqå¯¹åº”çš„ç­”æ¡ˆ\n",
    "        match = res.match   #åŒ¹é…åˆ°çš„ç›¸ä¼¼é—®å¥\n",
    "        faq = res.faq       #å¯¹åº”çš„æ ‡å‡†é—®å¥\n",
    "        score = res.score   #åŒ¹é…æ¨¡å‹åŒ¹é…çš„åˆ†æ•°ï¼Œå¯ä»¥è§†ä¸ºç½®ä¿¡åº¦\n",
    "        return answer, faq, score, match\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"æˆ‘æ˜¯ä½ çˆ¸çˆ¸\"\n",
    "    tt = Faq_inference()\n",
    "    print(tt.faq_answer(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb5bdc-a5fe-44a0-8811-11a425e7cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å›¾è¯†åˆ«\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "# è¯»å–csvæ–‡ä»¶(å‰nè¡Œ)\n",
    "def read(train_set_path, test_set_path):\n",
    "    data_1 = pd.read_csv('./faq.txt', sep='\\t', header=None, names=['query', 'faq', 'answer','tts'], usecols = ['query'])\n",
    "    data_1['class'] = 0\n",
    "    data_temp = pd.read_csv('qingyun_chat.txt', header=None, names=['query'])\n",
    "    data_temp['class'] = 1\n",
    "    data_2 = pd.read_csv('./chat.txt', sep='\\t', header=None, names=['match', 'query', 'answer','tts'], usecols = ['query'])\n",
    "    data_2['class'] = 1\n",
    "    data_3 = pd.read_csv('docqa.csv', header=None, names = ['query'])\n",
    "    data_3['class'] = 2\n",
    "    data_1 = data_1.drop_duplicates()\n",
    "    data_2 = data_2.drop_duplicates()\n",
    "    data = pd.concat((data_1, data_2, data_3, data_temp))\n",
    "    \n",
    "    data['query'] = data['query'].apply(lambda x: ' '.join(str(x)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['query'], data['class'], test_size=0.15)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# è®­ç»ƒæ‹Ÿåˆæ¨¡å‹\n",
    "def train(X_train, y_train):\n",
    "    vectorizer = TfidfVectorizer(max_features=800000, min_df=2, analyzer='word', token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", ngram_range=(1, 5)) \n",
    "    x_train = vectorizer.fit_transform(X_train)    \n",
    "    # æ„å»ºé€»è¾‘å›å½’æ¨¡å‹\n",
    "    logistic_model = LogisticRegression(C=3, dual=False) \n",
    "    # æ‹Ÿåˆ,è®­ç»ƒ\n",
    "    logistic_model.fit(x_train, y_train) \n",
    "    \n",
    "    tfidftransformer_path = 'models/tfidftransformer.pkl'\n",
    "    with open(tfidftransformer_path, 'wb') as fw:\n",
    "         pickle.dump(vectorizer, fw)        \n",
    "    joblib.dump(logistic_model, \"models/logisitic_model.pkl\") \n",
    "    return vectorizer, logistic_model\n",
    "\n",
    "\n",
    "# ä¿å­˜é¢„æµ‹ç»“æœ\n",
    "def save(test_data, y_test, save_path):\n",
    "    # ä¿å­˜ç»“æœ\n",
    "    y_test = y_test.tolist()  # æŠŠ(n,)çš„ä¸€ç»´çŸ©é˜µy_testè½¬åŒ–ä¸ºçŸ©é˜µåˆ—è¡¨å½¢å¼å­˜å‚¨\n",
    "    test_data['class'] = y_test + 1  # æ–°å»ºä¸€ä¸ªclassåˆ—,å­˜å…¥ç±»åˆ«å€¼(ç±»åˆ«è¿˜åŸ+1)\n",
    "    data_result = test_data.loc[:, ['id', 'class']]  # æ ¹æ®indexç´¢å¼•æ‰€æœ‰è¡Œå’Œid,classåˆ—\n",
    "    data_result.to_csv(save_path, index=False)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    chars = '*' * 20\n",
    "    '''1.è¯»å–æ•°æ®'''\n",
    "    # old_time = time.time() \n",
    "    X_train, X_test, y_train, y_test = read('./faq.txt', './chat.txt')\n",
    "    # current_time = time.time()\n",
    "    # print(chars + 'è¯»å–å®Œæˆ!ç”¨æ—¶{}s'.format(round(current_time-old_time,2)) + chars + '\\n')\n",
    "\n",
    "    '''2.è®­ç»ƒ'''\n",
    "    vectorizer, logistic_model = train(X_train, y_train)\n",
    "    \n",
    "    vectorizer = pickle.load(open('models/tfidftransformer.pkl', \"rb\"))\n",
    "    logistic_model = joblib.load('models/logisitic_model.pkl')\n",
    "    \n",
    "    '''3.é¢„æµ‹'''\n",
    "    x_test = vectorizer.transform(X_test)\n",
    "    y_pred = logistic_model.predict(x_test) # (n,)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(acc_score)\n",
    "    F1_score = f1_score(y_test, y_pred, average='macro')\n",
    "    print(F1_score)\n",
    "    \n",
    "    \n",
    "    #0:é—²èŠ ï¼Œ 1:å®¢æœï¼Œ 2: æ–‡æ¡£é—®ç­”ï¼ˆçŸ¥è¯†é—®ç­”ï¼‰\n",
    "    C2 = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "    # è®¡ç®—ç™¾åˆ†æ¯”\n",
    "    C2_normal = np.round(C2/np.sum(C2, axis=1).reshape(-1, 1), 2)\n",
    "\n",
    "    sns.set()\n",
    "    f, ax = plt.subplots()\n",
    "    sns.heatmap(C2_normal, cmap=\"YlGnBu\", annot=True, ax=ax) # heatmap\n",
    "\n",
    "    #ax.set_title('Normalized confusion maxtrix') # title\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.xaxis.set_ticklabels([0, 1, 2])\n",
    "    ax.yaxis.set_ticklabels([0, 1, 2])\n",
    "    \n",
    "    print('Over!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e295b-1bb5-42cb-9a92-0a6d20d04015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping = {-1:'Unknown', 0:'Faq', 1:'Chat', 2:'DocQA', 3:'Mutil-Turn'}\n",
    "import pickle\n",
    "import joblib\n",
    "query = 'å¥¹çš„å¦»å­å«ä»€ä¹ˆ'\n",
    "vectorizer = pickle.load(open('models/tfidftransformer.pkl', \"rb\"))\n",
    "model = joblib.load('models/logisitic_model.pkl')\n",
    "pred_class = model.predict(vectorizer.transform([' '.join(query)]))\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05e1d4-fa53-4eae-a2d9-820e132fd871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from protos.nlp.chat import chat_pb2\n",
    "from protos.nlp.chat import chat_pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "\n",
    "\n",
    "class Chat_inference(object):\n",
    "    def __init__(self):\n",
    "        super(Chat_inference, self).__init__()\n",
    "\n",
    "    def response(self, history_utts, query):\n",
    "        conn = implementations.insecure_channel(\"172.31.208.9\", 60005)\n",
    "        client = chat_pb2_grpc.ChatStub(channel=conn._channel)\n",
    "        res = client.GetChat(chat_pb2.ChatRequest(history=history_utts, query=query, session_id='1', robot_id=1), timeout=60)\n",
    "        answer = res.answer\n",
    "        score = res.score\n",
    "        query = res.query\n",
    "        sessionid = res.session_id\n",
    "        return answer, score, query, sessionid\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_infer = Chat_inference()\n",
    "    \n",
    "    history_utts = [{\"conversation\": [],'intent':[],'slots':[],'entities':[],'task_id':[]}]\n",
    "    query = \"ä½ å¥½\"\n",
    "    start_time = time.time()\n",
    "    results = chat_infer.response(history_utts, query)\n",
    "    end_time = time.time()\n",
    "    print('*' * 20 + 'å›å¤å®Œæˆ!ç”¨æ—¶{:.2f}s'.format(end_time - start_time) + '*' * 20)\n",
    "    print('results is:', *results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24aeb4-4d3d-4015-888e-71247deecfca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DocQa\n",
    "from protos.nlp.docqa import docqa_pb2\n",
    "from protos.nlp.docqa import docqa_pb2_grpc\n",
    "from grpc.beta import implementations\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "\n",
    "class DocQAInference(object):\n",
    "    def __init__(self, config):\n",
    "        self.host = config.get('host', \"172.31.208.4\")\n",
    "        self.port = config.get('port', 60005)\n",
    "        # self.port = config.get('port', 6082)\n",
    "        self.timeout = config.get('timeout', 60)\n",
    "\n",
    "    def docqa(self, index_id, query):\n",
    "        conn = implementations.insecure_channel(self.host, self.port)\n",
    "        client = docqa_pb2_grpc.DocqaStub(channel=conn._channel)\n",
    "        res = client.GetDocqa(docqa_pb2.DocqaRequest(index_id=index_id, query=query, session_id='1', robot_id=1), timeout=self.timeout)\n",
    "        return res\n",
    "\n",
    "#â€˜è¯—å¥ï¼šâ€˜å¤œæ¥é£é›¨å£°â€™çš„ä¸‹ä¸€å¥æ˜¯ä»€ä¹ˆâ€™\n",
    "if __name__ == \"__main__\":\n",
    "    query_index_list = [\n",
    "        (\"ä¸­å›½æœ‰å¤šå°‘ä¸ªçœ\", \"wiki\"),\n",
    "    ]\n",
    "    query_list = [item[0] for item in query_index_list]\n",
    "    index_list = [item[1] for item in query_index_list]\n",
    "    # index_list = index_list*25\n",
    "    # query_list = query_list*25\n",
    "    \n",
    "    inference = DocQAInference({})\n",
    "    inference.docqa(index_list[0], query_list[0])\n",
    "\n",
    "    import numpy as np\n",
    "    import time\n",
    "    query_lens = []\n",
    "    latency_list = []\n",
    "    for index, query in zip(index_list, query_list):\n",
    "        print(f\"index={index}\")\n",
    "        print(f\"query={query}\")\n",
    "        start_time = time.time()    \n",
    "        docqa_rst = inference.docqa(index, query)\n",
    "        time_cost = time.time() - start_time\n",
    "        print(f\"result={MessageToJson(docqa_rst, ensure_ascii=False, indent=2)}\")\n",
    "        latency_list.append(time_cost*1000)\n",
    "        query_len = len(query)\n",
    "        query_lens.append(query_len)\n",
    "        \n",
    "                \n",
    "    print(f\"mean of query len is {np.mean(query_lens)}\")\n",
    "    print(f\"mean of latency is {np.mean(latency_list)}ms\")\n",
    "    print(f\"0.95 of latency is {np.quantile(latency_list, 0.95)}ms\")\n",
    "    print(f\"0.99 of latency is {np.quantile(latency_list, 0.99)}ms\")\n",
    "    \n",
    "#ç‰¹æœ—æ™®çš„å¼Ÿå¼Ÿå«ä»€ä¹ˆ\n",
    "#ç¢§æ˜‚æ–¯çš„å‡ºç”Ÿåœ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe830df7-fd27-4e2e-88db-da678ef37c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"squad_data.json\", encoding='utf-8', errors='ignore') as json_data:\n",
    "     data = json.load(json_data, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5717-5efc-48d4-a0f5-47a858540adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a46c2-f5c7-41d7-968c-ce1bdec2f164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = []\n",
    "for i in range(len(data['data'])):\n",
    "    for j in range(len(data['data'][i]['paragraphs'])):\n",
    "        for z in data['data'][i]['paragraphs'][j]['qas']:\n",
    "            query.append(z['question'])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1bc91-f5b5-4a39-ba57-c45f1947fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca732f5-a0cb-42bc-ad5a-dc6564b7ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('docqa_1.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034ee3e-7b79-42fa-92d9-3877a0006279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ”¹å†™æ¨¡å—\n",
    "import grpc\n",
    "import sys\n",
    "from protos.nlp.rewrite import rewrite_pb2\n",
    "from protos.nlp.rewrite import rewrite_pb2_grpc\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "from grpc.beta import implementations\n",
    "\n",
    "class IUR_inference(object):\n",
    "    def __init__(self, config):\n",
    "        self.host = config.get('host', \"192.168.68.26\")\n",
    "        self.port = config.get('port', 60005)\n",
    "        self.timeout = config.get('timeout', 60)\n",
    "\n",
    "    def iur(self, history_utts, query):\n",
    "        conn = implementations.insecure_channel(self.host, self.port)\n",
    "        client = rewrite_pb2_grpc.ReWriteStub(channel=conn._channel)\n",
    "        res = client.GetReWrite(rewrite_pb2.ReWriteRequest(history=history_utts, query=query, session_id='1', robot_id=1), timeout=self.timeout)\n",
    "        new_query = res.new_query\n",
    "        score = res.score\n",
    "        return new_query, score\n",
    "\n",
    "    def query_write_strategy(self, query, conversation):\n",
    "        new_query_1, qr_score_1 = self.iur(conversation[:2], query)\n",
    "        new_query_2, qr_score_2 = self.iur(conversation[2:], query)\n",
    "        new_query_3, qr_score_3 = self.iur(conversation, query)\n",
    "        new_query, qr_score = new_query_1, qr_score_1\n",
    "        if qr_score_1 <qr_score_2:\n",
    "            new_query = new_query_2\n",
    "            qr_score = qr_score_2\n",
    "        print(new_query_1, qr_score_1)\n",
    "        print(new_query_2, qr_score_2)\n",
    "        print(new_query_3, qr_score_3)\n",
    "        if qr_score>0.8 and new_query!=query:\n",
    "            return new_query\n",
    "        return query\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # history_utts = [\"\", \"\", \"åŒ—äº¬å¯ä»¥åŠç†å·¥ä½œå±…ä½è¯ä¹ˆï¼Ÿ\", \"å¯ä»¥ï¼Œæ ¹æ®æ”¿åºœæ”¿ç­–è§„å®šï¼Œå…¥èŒæ»¡ä¸€å¹´åæ»¡è¶³æ¡ä»¶çš„å¯åŠç†ã€‚\"]\n",
    "    # query = \"å¤šä¹…èƒ½ä¸‹æ¥ï¼Ÿ\"\n",
    "    # history_utts = [\"\", \"\", \"æœ‰æ¾ä¸‹çš„å¹é£æœºå—\", \"æ²¡æœ‰å“¦äº²\"]\n",
    "    # query = \"é‚£æœ‰æˆ´æ£®çš„å—\"\n",
    "    # history_utts = [\"\", \"\", \"å¸®æˆ‘æ”¾å‘¨æ°ä¼¦çš„æ­Œ\", \"å¥½çš„ï¼Œæ­£åœ¨æ’­æ”¾å‘¨æ°ä¼¦çš„ç¨»é¦™\"]\n",
    "    # query = \"ç®—äº†ï¼Œä¸è¦æ”¾ä»–çš„ï¼Œæ”¾å¼ å­¦å‹çš„\"\n",
    "    # history_utts = ['','']\n",
    "    # history_utts = ['ä½ çŸ¥é“ç¢§æ˜‚æ–¯å—', 'è‚¯å®šçŸ¥é“å•Š','é‚£ä½ çŸ¥é“å¥¹çš„ç¬¬ä¸€å¼ ä¸“è¾‘å—','å±é™©çˆ±æƒ…']\n",
    "    # history_utts = ['ä½ çŸ¥é“ç¢§æ˜‚æ–¯å—','æˆ‘çš„æœ‹å‹æœ‰å¾ˆå¤šå•Šï¼Œä½ ä¹Ÿæ˜¯æˆ‘æœ‹å‹å•Šä½ å¿˜å•¦ï¼Ÿ', 'å¥¹çš„ç¬¬ä¸€å¼ ä¸“è¾‘æ˜¯ä»€ä¹ˆ', 'å±é™©çˆ±æƒ…']\n",
    "    history_utts = ['','','ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·', 'é˜´å¤©,æœ‰ç‚¹é—·çƒ­']\n",
    "    # history_utts = ['æ€ä¹ˆç”³è¯·è®¾å¤‡å‘¢', 'ITæ ‡å‡†åº“å­˜èµ„äº§åœ¨é‡‘è¶ç³»ç»Ÿ-äººäººèµ„äº§å¡«å†™ç”³è¯·ï¼›ITæ ‡å‡†é…ä»¶åœ¨é£ä¹¦-å·¥ä½œå°-å®¡æ‰¹å¡«å†™è€—æé¢†ç”¨æµç¨‹ï¼›éæ ‡å‡†åº“å­˜é…ä»¶åœ¨é‡‘è¶ç³»ç»Ÿå¡«å†™é‡‡è´­ç”³è¯·ã€‚', 'é‚£åŒ—äº¬å¯ä»¥åŠç†å±…ä½è¯å—', 'å¯ä»¥ï¼Œæ ¹æ®æ”¿åºœæ”¿ç­–è§„å®šï¼Œå…¥èŒæ»¡ä¸€å¹´åæ»¡è¶³æ¡ä»¶çš„å¯åŠç†ã€‚']\n",
    "    # history_utts = ['æ€ä¹ˆç”³è¯·è®¾å¤‡', 'ITæ ‡å‡†åº“å­˜èµ„äº§åœ¨é‡‘è¶ç³»ç»Ÿ-äººäººèµ„äº§å¡«å†™ç”³è¯·ï¼›ITæ ‡å‡†é…ä»¶åœ¨é£ä¹¦-å·¥ä½œå°-å®¡æ‰¹å¡«å†™è€—æé¢†ç”¨æµç¨‹ï¼›éæ ‡å‡†åº“å­˜é…ä»¶åœ¨é‡‘è¶ç³»ç»Ÿå¡«å†™é‡‡è´­ç”³è¯·ã€‚', 'åŒ—äº¬å¯ä»¥åŠç†å±…ä½è¯å—', 'å¯ä»¥ï¼Œéœ€ä¸Šå®¶å…¬å¸å‡å‘˜åç»‘å®šå…¬å¸å…³è”ç åç³»ç»Ÿæäº¤ææ–™å³å¯ã€‚']\n",
    "    # history_utts = ['ä½ å¥½', 'ä½ å¥½å‘€ï¼Œæˆ‘å¯ä»¥å¸®ä½ åšç‚¹å•¥å˜›', 'ä½ è®¤è¯†ç¢§æ˜‚æ–¯å—', 'é‚£å°±å¤šäº†å»äº†ï¼Œäº²äººæœ‹å‹åäººæ•°ä¸èƒœæ•°å‘ï¼']\n",
    "    # query=\"å¥¹çš„ç¬¬ä¸€å¼ ä¸“è¾‘æ˜¯ä»€ä¹ˆ\"\n",
    "    query = 'æ˜å¤©çš„å‘¢'\n",
    "    tt = IUR_inference({})\n",
    "    new_query = tt.query_write_strategy(query, history_utts)\n",
    "    # new_query, score = tt.iur(history_utts, query)\n",
    "    print(new_query)\n",
    "#0.9014959335327148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a32171-dca4-4f2a-b351-8fcf9082a506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import jieba.posseg as psg\n",
    "import jieba\n",
    "jieba.add_word('ä»Šå¤©', 100000)\n",
    "jieba.add_word('æ˜å¤©', 100000)\n",
    "jieba.add_word('åå¤©', 100000)\n",
    "jieba.add_word('ä¸Šåˆ',100000)\n",
    "jieba.add_word('ä¸‹åˆ',100000)\n",
    "\n",
    "def get_time_word(text):\n",
    "    time_res = []\n",
    "    word = ''\n",
    "    keyDate = {'ä»Šå¤©': 0, 'æ˜å¤©':1, 'åå¤©': 2}\n",
    "    for k, v in psg.cut(text):\n",
    "        if k in keyDate:\n",
    "            if word != '':\n",
    "                time_res.append(word)\n",
    "            time_res.append(k)\n",
    "        elif word != '':\n",
    "            if v in ['m', 't']:\n",
    "                word = word + k\n",
    "            else:\n",
    "                time_res.append(word)\n",
    "                word = ''\n",
    "        elif v in ['m', 't'] and 'å¼ ' not in k:\n",
    "            word = k\n",
    "    if word != '':\n",
    "        time_res.append(word)\n",
    "    return time_res\n",
    "\n",
    "def time_extract(text):\n",
    "    # text = text\n",
    "    time_res = []\n",
    "    word = ''\n",
    "    keyDate = {'ä»Šå¤©': 0, 'æ˜å¤©':1, 'åå¤©': 2}\n",
    "    for k, v in psg.cut(text):\n",
    "        if k in keyDate:\n",
    "            if word != '':\n",
    "                time_res.append(word)\n",
    "            word = (datetime.today() + timedelta(days=keyDate.get(k, 0))).strftime('%Yå¹´%mæœˆ%dæ—¥')\n",
    "        elif word != '':\n",
    "            if v in ['m', 't']:\n",
    "                word = word + k\n",
    "            else:\n",
    "                time_res.append(word)\n",
    "                word = ''\n",
    "        elif v in ['m', 't'] and 'å¼ ' not in k:\n",
    "            word = k\n",
    "    if word != '':\n",
    "        time_res.append(word)\n",
    "    result = list(filter(lambda x: x is not None, [check_time_valid(w) for w in time_res]))\n",
    "    final_res = [parse_datetime(w) for w in result]\n",
    "    return [x for x in final_res if x is not None]\n",
    "\n",
    "def check_time_valid(word):\n",
    "    m = re.match(\"\\d+$\", word)\n",
    "    if m:\n",
    "        if len(word) <= 6:\n",
    "            return None\n",
    "    word1 = re.sub('[å·|æ—¥]\\d+$', 'æ—¥', word)\n",
    "    if word1 != word:\n",
    "        return check_time_valid(word1)\n",
    "    else:\n",
    "        return word1\n",
    "    \n",
    "def parse_datetime(msg):\n",
    "    if msg is None or len(msg) == 0:\n",
    "        return None\n",
    "\n",
    "    if 'åˆ' not in msg and 'ç‚¹' not in msg:\n",
    "        dt = parse(msg, fuzzy=True)\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        m = re.match(\n",
    "            r\"([0-9é›¶ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+å¹´)?([0-9ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æœˆ)?([0-9ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[å·æ—¥])?([ä¸Šä¸­ä¸‹åˆæ™šæ—©]+)?([0-9é›¶ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+[ç‚¹:\\.æ—¶])?([0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+åˆ†?)?([0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç§’)?\",\n",
    "            msg)\n",
    "        if m.group(0) is not None:\n",
    "            res = {\n",
    "                \"year\": m.group(1),\n",
    "                \"month\": m.group(2),\n",
    "                \"day\": m.group(3),\n",
    "                \"hour\": m.group(5) if m.group(5) is not None else '00',\n",
    "                \"minute\": m.group(6) if m.group(6) is not None else '00',\n",
    "                \"second\": m.group(7) if m.group(7) is not None else '00',\n",
    "            }\n",
    "            params = {}\n",
    "\n",
    "            for name in res:\n",
    "                if res[name] is not None and len(res[name]) != 0:\n",
    "                    tmp = None\n",
    "                    if name == 'year':\n",
    "                        tmp = year2dig(res[name][:-1])\n",
    "                    else:\n",
    "                        tmp = cn2dig(res[name][:-1])\n",
    "                    if tmp is not None:\n",
    "                        params[name] = int(tmp)\n",
    "            target_date = datetime.today().replace(**params)\n",
    "            is_pm = m.group(4)\n",
    "            if is_pm is not None:\n",
    "                if is_pm == u'ä¸‹åˆ' or is_pm == u'æ™šä¸Š' or is_pm =='ä¸­åˆ':\n",
    "                    hour = target_date.time().hour\n",
    "                    if hour < 12:\n",
    "                        target_date = target_date.replace(hour=hour + 12)\n",
    "            return target_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "UTIL_CN_NUM = {\n",
    "    'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸‰': 3, 'å››': 4,\n",
    "    'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,\n",
    "    '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n",
    "    '5': 5, '6': 6, '7': 7, '8': 8, '9': 9\n",
    "}\n",
    "UTIL_CN_UNIT = {'å': 10, 'ç™¾': 100, 'åƒ': 1000, 'ä¸‡': 10000}\n",
    "def cn2dig(src):\n",
    "    if src == \"\":\n",
    "        return None\n",
    "    m = re.match(\"\\d+\", src)\n",
    "    if m:\n",
    "        return int(m.group(0))\n",
    "    rsl = 0\n",
    "    unit = 1\n",
    "    for item in src[::-1]:\n",
    "        if item in UTIL_CN_UNIT.keys():\n",
    "            unit = UTIL_CN_UNIT[item]\n",
    "        elif item in UTIL_CN_NUM.keys():\n",
    "            num = UTIL_CN_NUM[item]\n",
    "            rsl += num * unit\n",
    "        else:\n",
    "            return None\n",
    "    if rsl < unit:\n",
    "        rsl += unit\n",
    "    return rsl\n",
    "def year2dig(year):\n",
    "    res = ''\n",
    "    for item in year:\n",
    "        if item in UTIL_CN_NUM.keys():\n",
    "            res = res + str(UTIL_CN_NUM[item])\n",
    "        else:\n",
    "            res = res + item\n",
    "    m = re.match(\"\\d+\", res)\n",
    "    if m:\n",
    "        if len(m.group(0)) == 2:\n",
    "            return int(datetime.datetime.today().year/100)*100 + int(m.group(0))\n",
    "        else:\n",
    "            return int(m.group(0))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a037a0-6596-44da-8bc1-5b79f694c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def ner(query):\n",
    "    entities = []\n",
    "    slots = []\n",
    "    query = query.replace('ä»Šæ™š','ä»Šå¤©æ™šä¸Š').replace('æ˜æ—©','æ˜å¤©æ—©ä¸Š').replace('æ˜æ™š','æ˜å¤©æ™šä¸Š')\n",
    "\n",
    "    time = time_extract(query)\n",
    "    if time:\n",
    "        slots.append('time')\n",
    "        entities.append(time[0])\n",
    "    time_word = get_time_word(query)\n",
    "    for word in time_word:\n",
    "        query = query.replace(word, '')\n",
    "    print(query)\n",
    "\n",
    "    m = re.findall(r\"è®¢.*?[0-9ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]?å¼ ?ä»?([\\u4e00-\\u9fa5]{2,5}?)?(å‡ºå‘)?[åˆ°ï½œé£|å»ï½œ]([\\u4e00-\\u9fa5]{2,5}?)çš„\",query)\n",
    "    if m:\n",
    "        start_city = m[0][0]\n",
    "        end_city = m[0][2]\n",
    "        if start_city:\n",
    "            slots.append('start_city')\n",
    "            entities.append(start_city)\n",
    "        if end_city:\n",
    "            slots.append('end_city')\n",
    "            entities.append(end_city)\n",
    "\n",
    "        print(start_city)\n",
    "        print(end_city)\n",
    "    else:\n",
    "        pass\n",
    "    return slots, entities\n",
    "    \n",
    "ner('æ˜æ™šå…«ç‚¹')\n",
    "# ner('è®¢ä¸€å¼ æ˜æ™šå…«ç‚¹çš„æœºç¥¨')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27090e6-c728-4693-b418-cce61c6c14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'å¸®æˆ‘è®¢ä¸€å¼ æ˜æ™šå…«ç‚¹ä¸Šæµ·åˆ°æ·±åœ³çš„æœºç¥¨'\n",
    "print(text1, time_extract(text1), sep=':')\n",
    "\n",
    "# text2 = 'é¢„å®š28å·çš„æˆ¿é—´'\n",
    "# print(text2, time_extract(text2), sep=':')\n",
    "\n",
    "# text3 = 'æˆ‘è¦ä»26å·ä¸‹åˆ4ç‚¹ä½åˆ°11æœˆ2å·'\n",
    "# print(text3, time_extract(text3), sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc8c6f-2914-432d-912c-d16499502b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æƒ…ç»ªè¯†åˆ«\n",
    "\n",
    "import grpc\n",
    "from protos.nlp.faq import faq_pb2, faq_pb2_grpc\n",
    "from protos.nlp.emotion import emotion_pb2, emotion_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('172.31.208.10:58999')\n",
    "texts = ['ä½ çœŸæ£’']\n",
    "\n",
    "# emotion\n",
    "emotion_client = emotion_pb2_grpc.EmotionStub(channel)\n",
    "for query in texts:\n",
    "    resp = emotion_client.GetEmotion(emotion_pb2.EmotionRequest(query=query, session_id='1', robot_id=2))\n",
    "    # print(resp.answer.happy, resp.answer.sad, resp.answer.angry, resp.answer.disgust,\n",
    "    #       resp.answer.fear, resp.answer.surprise, resp.answer.none)\n",
    "    print(resp)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01108689-0d3d-4c6e-b3b2-9e0182e6c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  57491\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2813381 lr:  0.000000 avg.loss:  0.068367 ETA:   0h 0m 0s\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17536, 0.9757641423357665, 0.9757641423357665)\n",
      "ä¸–ç•Œ æœ€é«˜ å±±å³° æ˜¯ å“ªåº§\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#é¢†åŸŸåˆ†ç±»æ¨¡å‹ï¼ˆFasttextï¼‰\n",
    "import fasttext\n",
    "import jieba\n",
    "# from gensim.models import FastText\n",
    "jieba.add_word('ä½ çŸ¥é“',100000)\n",
    "\n",
    "model = fasttext.train_supervised(input = r\"./train.txt\", label_prefix=\"__label__\", epoch=10, verbose=2, lr=0.04, loss=\"softmax\")\n",
    "\n",
    "model.save_model(r\"models/model.bin\")\n",
    "\n",
    "stop_words = ['çš„','äº†','å—','å•Š','æ‹‰','å‘¢','ä½ çŸ¥é“','å‘Šè¯‰', 'è¯·é—®']\n",
    "\n",
    "def word_segment(text):\n",
    "    seg_list = jieba.cut(text)\n",
    "    word_list = []\n",
    "    for word in seg_list:\n",
    "        if word not in stop_words:\n",
    "            word_list.append(word)\n",
    "    line = \" \".join(word_list)\n",
    "    print(line)\n",
    "    return line\n",
    "\n",
    "# é¢„æµ‹\n",
    "model = fasttext.load_model(r\"models/model.bin\")\n",
    "print(model.test(r\"./test.txt\")  )\n",
    "\n",
    "# text = 'hiç©†è'\n",
    "text = 'ä¸–ç•Œæœ€é«˜çš„å±±å³°æ˜¯å“ªåº§'\n",
    "print(model.predict(word_segment(text))[0][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943ca756-1fa3-48f1-b2b5-ac605ecc9960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/0p/cjvwlkfs59d5t070gww_cp300000gn/T/jieba.cache\n",
      "Loading model cost 0.281 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#æ–‡ä»¶é¢„å¤„ç†\n",
    "import jieba\n",
    "jieba.add_word('ä½ çŸ¥é“',100000)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Faq\n",
    "data_1 = pd.read_csv('./faq.txt', sep='\\t', header=None, names=['query'])\n",
    "data_1['class'] = 0\n",
    "\n",
    "#é—²èŠ\n",
    "data_temp = pd.read_csv('qingyun_chat.txt', header=None, names=['query'])[:50000]\n",
    "data_temp['class'] = 1\n",
    "data_2 = pd.read_csv('./chat.txt', sep='\\t', header=None, names=['query'])\n",
    "data_2['class'] = 1\n",
    "\n",
    "#æ–‡æ¡£é—®ç­”\n",
    "data_3 = pd.read_csv('docqa_2.csv', header=None, names = ['query'])\n",
    "data_3['class'] = 2\n",
    "\n",
    "data_2 = data_2.drop_duplicates()\n",
    "data_temp = data_temp.drop_duplicates()\n",
    "data = pd.concat((data_1, data_2, data_3, data_temp))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['query'], data['class'], test_size=0.15)\n",
    "\n",
    "stop_words = ['ä½ çŸ¥é“','çš„','äº†','å—','å•Š','æ‹‰','å‘¢','è¯·é—®','å•¥']\n",
    "\n",
    "def word_segment(text):\n",
    "    seg_list = jieba.cut(text)\n",
    "    word_list = []\n",
    "    for word in seg_list:\n",
    "        if word not in stop_words:\n",
    "            word_list.append(word)\n",
    "    line = \" \".join(word_list)\n",
    "    return line\n",
    "\n",
    "\n",
    "#2: æ–‡æ¡£é—®ç­”ã€‚1:é—²èŠ\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(X_train)):\n",
    "        f.write('__label__'+str(y_train.values[i])+'\\t'+word_segment(X_train.values[i])+'\\n')\n",
    "        \n",
    "f.close()\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(X_test)):\n",
    "        f.write('__label__'+str(y_test.values[i])+'\\t'+word_segment(X_test.values[i])+'\\n')\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a689f-9008-4263-93d3-d461a937df8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import grpc\n",
    "from protos.nlp.dm import dm_pb2\n",
    "from protos.nlp.dm import dm_pb2_grpc\n",
    "\n",
    "def run():\n",
    "    # è¿æ¥ rpc æœåŠ¡å™¨\n",
    "    channel = grpc.insecure_channel('localhost:50051')\n",
    "    # è°ƒç”¨ rpc æœåŠ¡\n",
    "    stub = dm_pb2_grpc.DmStub(channel)\n",
    "    history = {}\n",
    "    while True:\n",
    "        text = input()\n",
    "        response = stub.GetDm(dm_pb2.DmRequest(text=text, history=history,session_id='1', robot_id = -1))\n",
    "        print(\"Dm client received: \" + response.answer)\n",
    "        history = response.history\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683006f0-6b4e-435f-86dc-2ab62325aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d26ac-2aa3-49e1-a7db-15bc361bc8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse \n",
    "sys.path.append('./DM/')\n",
    "sys.path.append('./DM/models/')\n",
    "from DmClass import *\n",
    "\n",
    "text = 'å¸®è®¢ä¸€å¼ ä»æ·±åœ³åˆ°åŒ—äº¬çš„æœºç¥¨'\n",
    "history = {'conversation':['','']}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--robot_type', default='', choices=['DH', 'KF'], type=str, help='Robot type, DH or KF')\n",
    "parser.add_argument('--chat_repo', default=1, choices=[0, 1], type=int, help='which repo used by chat_infer module')\n",
    "parser.add_argaument('--skill_type', default='DH', choices=['DH', 'KF'], help='What type of multi-turn capability the robot has')\n",
    "parser.add_argument('--FAQ', action=\"store_true\", default=False, help='if use the faq server')\n",
    "parser.add_argument('--QR', action=\"store_true\", default=False, help='if use the query rewrite server')\n",
    "\n",
    "parser.add_argument('--qr_ip', default='192.168.68.26', type=str)\n",
    "parser.add_argument('--qr_port', default='60005', type=str)\n",
    "parser.add_argument('--faq_ip', default='172.31.208.10', type=str)\n",
    "parser.add_argument('--faq_port', default='58999', type=str)\n",
    "parser.add_argument('--docqa_ip', default='172.31.208.4', type=str)\n",
    "parser.add_argument('--docqa_port', default='60005', type=str)\n",
    "parser.add_argument('--chat_ip', default='172.31.208.9', type=str)\n",
    "parser.add_argument('--chat_port', default='60005', type=str)\n",
    "parser.add_argument('--emotion_ip', default='172.31.208.10', type=str)\n",
    "parser.add_argument('--emotion_port', default='58999', type=str)\n",
    "args = parser.parse_args(args=[])\n",
    "dm  = DmClass(args)\n",
    "dm.load_data(text, history, '1', -1)\n",
    "\n",
    "ans, history, emotion = dm.response()\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd57f648-0131-4ff3-bfcd-f25292f3568b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ä»Šå¤©å¤©æ°”æ€ä¹ˆæ · \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ä»Šå¤©å¤©æ°”æ€ä¹ˆæ · \n"
     ]
    },
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses\"\n\tdebug_error_string = \"{\"created\":\"@1679289779.143625000\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3094,\"referenced_errors\":[{\"created\":\"@1679289779.143624000\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/lib/transport/error_utils.cc\",\"file_line\":163,\"grpc_status\":14}]}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         history \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mhistory\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext:\u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetDm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdm_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDmRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobot_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDm client received: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response\u001b[38;5;241m.\u001b[39manswer,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m history \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/grpc/_channel.py:946\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    938\u001b[0m              request,\n\u001b[1;32m    939\u001b[0m              timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m              wait_for_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    943\u001b[0m              compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    944\u001b[0m     state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    945\u001b[0m                                   wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dm/lib/python3.9/site-packages/grpc/_channel.py:849\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses\"\n\tdebug_error_string = \"{\"created\":\"@1679289779.143625000\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3094,\"referenced_errors\":[{\"created\":\"@1679289779.143624000\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/lib/transport/error_utils.cc\",\"file_line\":163,\"grpc_status\":14}]}\"\n>"
     ]
    }
   ],
   "source": [
    "#DM1.1\n",
    "#é—²èŠã€Faqã€æ–‡æ¡£é—®ç­”ï¼ˆwikiã€mitaï¼‰\n",
    "import grpc\n",
    "from protos.nlp.dm import dm_pb2\n",
    "from protos.nlp.dm import dm_pb2_grpc\n",
    "\n",
    "def run():\n",
    "    # è¿æ¥ rpc æœåŠ¡å™¨\n",
    "    channel = grpc.insecure_channel('172.31.208.4:55555')\n",
    "    # è°ƒç”¨ rpc æœåŠ¡\n",
    "    stub = dm_pb2_grpc.DmStub(channel)\n",
    "    \n",
    "    #ç¬¬ä¸€è½®\n",
    "    history = {}\n",
    "    while True:\n",
    "        text = input()\n",
    "        print('text:',text)\n",
    "        response = stub.GetDm(dm_pb2.DmRequest(text=text, history=history, session_id='1', robot_id = -1))\n",
    "        print(\"Dm client received: \" + response.answer,'\\n')\n",
    "        history = response.history\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923320c-11f9-4516-9b55-ca664e6bab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLM-6Bï¼šç¢§æ˜‚ä¸(Bèƒ¸ç—›)æ˜¯ä¸€ä½çŸ¥åçš„ç¾å›½å¥³æ­Œæ‰‹ã€è¯æ›²åˆ›ä½œäººå’Œæ¼”å‘˜ã€‚å¥¹å‡ºç”Ÿäº1982å¹´6æœˆ13æ—¥,å‡ºç”Ÿåœ°ä¸ºç¾å›½åŠ åˆ©ç¦å°¼äºšå·æ´›æ‰çŸ¶å¸‚ã€‚ç¢§æ˜‚ä¸ä»¥ç‹¬ç‰¹çš„å£°éŸ³ã€å‡ºè‰²çš„èˆè¹ˆæŠ€å·§å’Œå¤šæ ·åŒ–çš„éŸ³ä¹é£æ ¼è€Œé—»å\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests, random, json\n",
    "def tupe_conversation(conversation):\n",
    "    result = []\n",
    "    for i in range(0, len(conversation), 2):\n",
    "        result.append((conversation[i], conversation[i+1]))\n",
    "    return result\n",
    "\n",
    "query = \"ä½ çŸ¥é“ç¢§æ˜‚æ–¯å—ï¼Œå›å¤ä¸­ä¸è¦æœ‰è‹±æ–‡å­—æ¯\"\n",
    "conversation = ['ä½ å¥½', 'ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚']\n",
    "\n",
    "len_now = len(query)+sum(len(text) for text in conversation)\n",
    "chat_params = {\"max_length\":len_now+50, \"num_beams\":1, \"do_sample\":True, \"top_p\":0.7, \"temperature\":0.95,\n",
    "                \"history\":tupe_conversation(conversation)}\n",
    "chat_params = json.dumps(chat_params, ensure_ascii=False)\n",
    "\n",
    "session_hash = \"\".join(random.sample('zyxwvutsrqponmlkjihgfedcba1234567890',9))\n",
    "resp = requests.post(url=\"http://172.31.208.4:60015/run/predict/\", json={\"data\": [query, None, chat_params], \"fn_index\": 0, \"session_hash\": session_hash}).json()\n",
    "print(re.match('<p>(.*?)</p>\\n', resp['data'][4]['value'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff666dec-19b5-4010-9a1d-496b1335f74e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatGLM-6Bï¼šç¢§æ˜‚ä¸(Bailey Cyrus)æ˜¯ä¸€ä½è‘—åçš„ç¾å›½å¥³æ­Œæ‰‹ã€æ¼”å‘˜å’Œè¯æ›²åˆ›ä½œäººã€‚å¥¹å‡ºç”Ÿäº1982'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('<p>(.*?)</p>\\n', resp['data'][4]['value'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dddb87-7643-42f9-8e4a-6e3d982d59f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
